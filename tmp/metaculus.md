**Speaker A:**  You know, we do edit the show, so any, ~~uh,~~ interruptions, you need to take a break, somebody comes in, no problem. ~~Um,~~ we will also offer you, if you'd like, the opportunity, but not obligation, to review our edit before we publish. Most, ~~um,~~ people don't take us off on that, but it's totally up to you. ~~Um,~~ some people have ~~like,~~ corporate requirements where they have to, ~~um.~~ **Speaker B:**  Sounds good. **Speaker A:**  ~~So it's a standard offer.~~ ~~Um, have you seen my outline? I assume that made way to you.~~ **Speaker B:**  ~~I did, yeah. Yeah. Look it. Cool. So we can certainly previous chapters you've done too, so I'm excited to be here for it.~~ **Speaker A:** ~~Great. Thank you.~~ **Speaker B:** ~~Uh, aw.~~ **Speaker A:**  ~~Um,~~ we can certainly deviate from that. If there's anything you want to add or subtract, ~~you know, whatever, it,~~ it's all good, but at least that's a good starting point, hopefully. **Speaker B:**  ~~I think it looks good. Um,~~ I love going organic and just seeing where the conversation takes us. ~~Um,~~ I do think talking a little about approaches on such a technical alignment is something that I find pretty interesting. Like before metaculous, I was, ~~um,~~ leading AI objectives in institute, which is a research lab that focuses on context in which alignment manifests today. And I find that to be quite interesting as a counterweight. ~~Um,~~ we can talk about that a little bit. ~~Um,~~ but yeah, overall, ~~um,~~ I feel good. First time I'm doing a podcast also, so ~~I, um,~~ am interested in seeing how that goes. **Speaker A:** ~~Love it.~~ I take a good amount of pride, actually, in having a lot of first time podcast guests, so that's cool. ~~M cool.~~ **Speaker B:**  ~~Um, so, yeah,~~ maybe we'll just start. **Speaker A:**  A little bit with that at the top. ~~Yeah.~~ **Speaker B:**  ~~Um,~~ because you said we will edit it, I guess I can ~~like,~~ say, can we do ~~a ret take~~ of a part ~~or something?~~ And that's fine. **Speaker A:**  Yeah, it's totally fine. ~~Um, we even, in theory, have AI tools that can help us with that to a certain degree now, although we do have, uh, a human, uh, oversight still in place for now, at least.~~ **Speaker B:**  I think where the tech is at right now requires this. So I think that sounds about right. **Speaker A:**  ~~There is actually later today I have an interview with the CEO of Descript, which we use for editing, and they have recently launched a set of new features that collectively they call Underlord, which I think is funny. It's kind of obviously a play on overlord.~~ ~~The idea is the AI, you are the overlord and the AI is the underlord. Um, but they do have a like, retake removal where you can do these retakes and then just say, like, remove my retakes and it will, in theory, go through and cuddle that stuff.~~ ~~So, yeah, it's new. I wouldn't say we've like, battle tested it just yet, but. So, yeah, maybe put a little bit of your kind of background at the top and then, um, that can inform other things we talked about. But does that sound good to you? Just start with that.~~ **Speaker B:**  ~~That sounds good.~~ ~~I can also get to the background more from, like, how did I start focusing on forecasting? I like the order of questions you put on on, you know, about you fire.~~ ~~And then we can go from there and I can share, you know, what my interests are in the space and go from there.~~ **Speaker A:**  Okay, cool. Can you pronounce your name for me so I can make sure I'm saying it correctly? **Speaker B:**  It's dare. Like truth or dare? **Speaker A:**  Dare. Okay. **Speaker B:**  And last name to run. **Speaker A:**  Dare to run. Cool. ~~Um, well, let me get.~~ I kind of lost my voice at an event this past weekend, and it's mostly back, but it's not a hundred percent back, so I apologize for, ~~um,~~ not sounding my very best, but here we go. Dertron, CEO of Metaculus. Welcome to the cognitive revolution. **Speaker B:**  Thank you. Big fun here. **Speaker A:**  Thank you. That's ~~kind of~~ you to say. I'm excited for this conversation.



**Speaker B:** I realized this is actually quite instrumental in how we look at the world. So, um, that is what brought me towards thinking about more forecasting. Cool. **Speaker A:**  Just two. Rewind back in time for a second to the 2016 era. Did any of that stuff work? Were you able to make anything with technology that was available at the time that you thought yielded any practical utility? **Speaker B:**  ~~Um,~~ ~~yes. Well,~~ back then I was working actually with, ~~um,~~ Dan Jfsky, ~~um,~~ on a question at Stanford on, ~~um,~~ federal agency regulatory feedback. So the FCC net neutrality debate was just taking off, and there was 2 million comments that were submitted to the federal agency. And at that point this seemed like, wow, this is the largest data set that was ever collected of, ~~you know,~~ raw text. Now, it's ridiculously small, but at that point there was this question of, ~~okay,~~ how should a central agency pay attention to this? There was a lot of interesting questions. Like, a lot of the submissions were form letters that are copy paste. But the fact that it is copy paste does not make it illegitimate. ~~You know,~~ I will read, ~~you know,~~ a campaign and say, yeah, I agree with this, I want to send this. How should you take that into account compared to, ~~you know,~~ something that is bot generated compared to something that is you? A genuine opinion that someone has taken time, but it's not necessarily sophisticated and you can see that it has shortcomings. ~~And, um,~~ in my thesis work, I was looking at this both from a legal perspective and also technical perspective. ~~You know,~~ what are the expectations from, ~~you know,~~ EPA and EPA perspective on just the us legislation, but also, how can we have natural languageing augment this? ~~Um,~~ and funny enough, at that point I was thinking a lot of questions of good hearting. Like, every time we start focusing on a specific, narrower metric, it ceases to be a good measure. And this was very apparent in being able to claim that you're looking at the public. ~~Um,~~ things we tried were so simple, and yet they seemed like, ah, oh, this is a paradigm shift. ~~Um,~~ just like simple clustering strategies, unsupervised learning topic modeling. ~~Um,~~ and even that went a long way because people had no idea, and now we can do much more sophisticated versions and people still are like, people assume that we don't need to ask these questions because we cannot do large scale qualitativeveys. So we keep going back to Likert scale. So I guess the crusade I've been on for a long while is, can we go past, ~~uh,~~ how happy are you, one to seven, or should we build a road from a to b or a to c. I'm like, maybe we need to build a bridge instead. And if you don't let people express that, you're just not going to get that opinion. So how do we get, ~~um,~~ machine learner systems optimizers to be able to understand? ~~Uh,~~ there is something qualitative that I need to elicit that even the speaker might not be sure what they want. Like, how can we give room for that human flexibility in these misschions? ~~I cannot hear. Sorry.~~ **Speaker A:**  ~~There's a little construction outside my, uh, window, so I'm muting the mic sometimes, but.~~ ~~And then I've got to remember to unmute it. Um,~~ so your comment on goodharting definitely resonates with me, particularly today. I was just, ~~uh,~~ having some back and forth on Twitter about the new GPT four o mini model, which is coming in ahead of claud 3.5 sonnet on the, ~~uh,~~ LMC leaderboard. And that raises the question. It seems like the general consensus among people like me who are obsessed with the latest AIs'and, ~~kind of~~ have their own individual opinions, seems to be that 3.5 sonnet is the best. ~~Um,~~ certainly we ~~all would~~ all tend to think it's better than GB 400 mini, and yet there it is, ~~uh,~~ ahead in terms of the votes that it's getting.

**Speaker A:** So this has raised all sorts of why is this happening? Do we trust this thing anymore? ~~Um,~~ is it just about formatting? Is formatting. If it is mostly about formatting, does that mean it's like, illegitimate in some way, or does that mean we should really think more about the importance of formatting? ~~Um,~~ but yeah, ~~it's very.~~ As always, it's extremely difficult to pin much down if you really want to do it, ~~um,~~ with any real rigor. So that, ~~um,~~ is maybe a good bridge toward. And certainly the same is even more true about the future. So I've been interested in this space of forecasting for a long time. I actually participated in one of the original, ~~um,~~ tetlock organized super forecasting tournaments maybe 15 years ago now. ~~Uh,~~ Tyler Cowen posted, ~~ah,~~ an invitation to participate, and I was like, I'll try it in my hand at that. I did reasonably well. I wouldn't say, ~~um,~~ I didn't top the leaderboard, but I came out feeling like I had at least somewhat of a knack for it. And I've always found that the sort of Robin ~~Hanseson~~ perspective on this seems like a much better way than sor of highest paid person's opinion or whatever kind of prevails in organizations. ~~Um,~~ that's always been compelling to me, but it seems like generally speaking, we still don't see a ton of forecasting deployments in the world, I guess. What would your analysis be of kind of where we are in forecasting, where you think the most notable deployments are, and why we don't see more than we do today? **Speaker B:** ~~ Right.~~ I want to answer from the previous point you made around good hearting, because I think this actually goes towards there and ~~you know,~~ which model is better? Define better. Right. Better is incredibly content specific. Like even in just a small world of things I am trying to do personally within metaculous clau like 3.5 is more useful for me on question writing, but not necessarily fact retrieval. And it's a very narrow thing. And this current version of these models seem like, ~~you know,~~ one of them has an easier knack towards one thing without interrupting, without self censoring, so I can move forward faster. I think the core of forecasting also relates to this in terms of how can this process actually be useful? Is one thing that is different from, ~~you know,~~ will it score good on these benchmarks? Like in this one specific competition, will, ~~you know,~~ 4.0 score better? ~~Like,~~ that is a much narrower thing. In a way. Usefulness is almost always anecdotal and I think we are not paying enough attention to that. And I think the same as applicable for ~~forecs.~~ Well, there has been so many tournaments like latest, know, like ACX ~~uh,~~ 2023 tournament is interesting for how accurate the forecasts have gotten. ~~Um,~~ but to me, okay, how is this going to actually be useful for an end user is a different question. And I think this is the approach that I really want to focus on with metaculous for the upcoming sprint is, okay, we, I have a couple of validations that, ~~you know,~~ I feel satisfied with. What are those? One wisdom of the crowd works. It is interesting that it works, but ~~when you aggregate a number of people, and we will go into it,~~ when you aggregate a number of bots, ~~um,~~ that are trained with LLMs or what have you, it seems to be able to draw an accurate picture of the future. And we have enough case studies of this. Why is this not more commonly used? I think usefulness is a whole different paradigm than accuracy, and this is really what I want to focus on. What do we want ~~Toa do~~ on this front? Like for example, can we identify critical inflection points towards future states? Is a different question than just is your forecast accurate? What is the shape of the world? That we want to live in. What are entities? ~~You know,~~ these can be federal agencies, like my prior work.

**Speaker B:** This can be philanthropists, this can be corporations, nation states. The framework of forecasting so far, especially from a te talking lens, I think has not focused on how will this be actually useful, but has more so focused on can we improve the accuracy. I would like to move forward towards coming up with versions of forecasting deployments that actively pay attention to how will the decision maker take this into account? What are things that are within their space of levers? This is why I think forecasting in context of the humans that are trying to live a better life is extremely important. ~~Um,~~ I can share more examples on this front, but I'm curious where ~~you'to go.~~ But, ~~um,~~ with this, like pretty much our research agenda for metaculous, for the next chapter, for the next, ~~um,~~ two quarters heavily focuses on a lot of experimentation on these kinds of questions. **Speaker A:**  Yeah, interesting. ~~Does that look like basically~~ conditional. **Speaker B:**  ~~Markets.~~ **Speaker A:**  ~~Um, being kind of~~ the ~~first thing, I mean,~~ that's something that Robin Henson has talked about for a long time, and I don't see much, but we've just lived through a moment in history where it seems like something like that was pretty motivating to the current president of the United States, ~~who I, uh, think was,~~ if reporting and general intuition are correct, seems like he was not sold on the idea that somebody else had a better chance than him until he saw polling data suggesting that and then was ~~kind of~~ like, ~~well,~~ I guess, yeah, if somebody else has that much of a better chance, maybe I should step aside. ~~Um,~~ is that the sort. Obviously play that out a million different ways for a million different context. But is that the kind of next big paradigm? **Speaker B:**  I think that is an example that basically is, ~~uh,~~ a very simplified ~~and~~ somewhat obvious to the entire population, except the decision makers themselves for longer than it should have, in my opinion, state of affairs. With that said, the real cases that I'm interested in is much more sophisticated than this, right. That heavily looks at public opinion and is more, I may say, like vibes based rather than data driven. And I'm interested in really pushing this edge further. Say we have $50 million to allocate towards making the world better. Pick your challenge. You reducing microplastics, climate related risks, AI related risk. How should we allocate these resources to get to a better world? Requires us to build world models. I think forecasts around what are specific outcomes we can get to if we take an intervention point is very helpful. And the simplest thing we can do is exactly what you said on you come up with conditional forecast. If we take lever x, will that bring us to a better future? If not, what will it be like? ~~Um,~~ and if we see high divergence here, that is great. Now I want to push this much further. We can already do this. ~~Um,~~ but ~~throughuth~~ test conditional questions are somewhat hard to answer. It's hard to think in that framework. So can we build tools, be it, ~~you know,~~ LLM driven or discourse driven, or come up with ways in which people can collaborate, that enable conditional forecasts to be more useful? This is one avenue. One of the things I want to do in, ~~uh,~~ the near future is launch a series of minitacul. This is the name we came up with. ~~Think of it like~~ subreddits, which is metaculous instances. This will come in context of us open sourcing metaculus, and we hope that many of these will spring in the next quarter. ~~Um,~~ each Mintaculus will be focused on a specific question, a goal oriented question, such as we are trying to reduce microplastics, for example, or we're trying to reduce homelessness in San Francisco. This is geared towards disaster response or consequences of research avenues and context of AI. ~~I would want every question in the minuteacul to be able to serve.~~

**Speaker B:** If we are able to answer this question, it bubbles up to the parent, and then we can see which of these questions have the highest value of information. Which of these questions actually inform us to say, oh, it looks like this intervention is going to be able to drive much further value. I would want us to identify critical inflection points like, is there a specific moment in which the world models see to diverge? Are we able to extract schools of thought in the context of forecasting by seeing, ~~oh, uh,~~ this group of forecasters seem to be much more in accordance over multiple forecasts. Why is there world modern divergent? Can we double down on finding more questions that can help us excavate that? These kinds of research questions go way beyond what metaculous has aimed for so far. This is actively trying to build a world model in an enclosed world space that is metaculous with a spec goal. And I find this to be ~~really,~~ really interesting. ~~Like,~~ the kinds of things I would love to do is, for example, can we find short term proxies and heavily forecast on both? Is this short term proxy good? And how is the short term proxy panning out? ~~Um,~~ the way we landed with the fab, the, ~~uh,~~ forecasting AI benchmark is basically guided from this. Also in a way, ~~um,~~ we can frame the presidential debate through this lens. I think we as a civilization need to think much more from this lens. Like this is a version of a cognitive revolution where we change how people are thinking about the future. In a way, we're enabling them to coordinate between their world models. I still see forecast aggregation to be fairly low fidelity to be able to coordinate world models, but I think we can use that as a building block, as a primitive, and come up with much better world models. **Speaker A:**  So let's maybe take one step back just for those who maybe don't have so much experience with metaculous as it exists today. ~~Uh,~~ I'd be interested to hear just like for starters, how do you describe it to somebody who's never seen it before? ~~Um,~~ would you call it a prediction market? Would you call it a forecasting platform? ~~Um,~~ how does it fit it? Because there's like a few of these now, right, with people would be familiar with certainly just online betting platforms, but also there's like manifold and polymarket that I see ~~kind of, you know,~~ shared them around. How would you describe the overall product today, and how would you distinguish it from some of the other things that are out there? **Speaker B:**  Mhm. Metaculous aggregates forecasts. It's a forecasting platform and an aggregation engine. I would not call metaculous a prediction market. The goal of Metaulus is to bring a level of epistemic security towards how we envision the future to be. We can call it Wikipedia about the future, and it is a space in which multiple people can see how they foresee the future to play out and the critical discussions to take place there. Um, it works with predictions as its core block, but it does not have a monetary incentive. Um, it behaves quite differently than prediction markets as a result of that. And some of those differences are the reasons why, in my opinion, Metaculus has been both more accurate and also more rigorous compared to a lot of other spaces. Um, uh, I can shed more light into that. For example, on a prediction market, the, uh, participants are buying and selling contracts that are based on the future outcome of an event, right? So you place bets and the result is a zero sum question. For example, if the current forecast is at 60% and you think it is 60%, you don't have an incentive to add that information in because it only can get worse from there. While on metaculous, this is not the case. Um, there's no financial incentive, and instead we are comparing all the forecasters accuracy and track record through time.

**Speaker B:** And, ~~um,~~ this creates a different set of incentives that I believe are actually much more productive towards something that looks like Wikipedia about the future, where people are one incentivized share what their world model is. Two, people are incentivized to participate in forecasts and not think about de risking or spreading, but instead focus on, ~~you know,~~ what is the world model that I have that I can share. So our scoring mechanisms are different with respect to prediction markets as a result of that, too, because the reward mechanism is that you reward a forecaster over time for their calibration across many predictions and for their accuracy. ~~Um,~~ we also have a metric that we call the community prediction. This is interesting because this is where you can see everyone that has predicted on this question. And we also have weighted averages depending on your past success on your track record, and the recency for sooner forecasts ~~thand~~ to be better for, you have more information. So using these, we actually end up in an ecosystem that is much more collaborative and much more grounded and good epistemicics. And this brings higher rigor as well. So the questions we forecast tend to be more like, ~~you know,~~ carbon emissions on this d, rather than very personal questions, ~~uh,~~ on you. Who am I gonna be dating with next? And I like that, too. ~~You know,~~ there's a market for that, but I like that there is a space for focusing on the rigor that is actually beneficial for the world. So that is what I find distinct and attractive about metaculous. **Speaker A:**  Gotcha. So it seems like the big thing there is the incentives, whereas on a more market oriented platform, I am looking for things that are wrong so I can make money here. I am looking for just opportunities where I can share something with the community that hopefully brings the overall, ~~uh,~~ community forecast toward a more accurate. **Speaker B:**  Right. But on top of that, I will add, there is a financial incent~~iment,~~ metaculous, too, as in, if you have very good track record as a forecaster, we will hire you as a pro forecaster to go engage on many specific paid client works. And these range from federal agencies to large retailers, to hedge funds, think tanks that have much narrower questions. These questions don't make their way to the public forecast that we have, and they will be paid engagements, and we will also pay the pro forecasters for their reasoning so that they can actually explain why they see the forecast to be this way. Just to echo back to a thing we were talking about, I think one of the core projects that we have for metaculous is going to be reasoning transparency. The forecast on, say, I'm making up an example, ~~um,~~ say we'thinking about the taiwanese sovereignty. This was the context in which this came up. ~~Um,~~ I was in Taiwan recently for a conference, and we were talking about, ~~um,~~ will Taiwan, ~~you know,~~ electricity grid be, ~~um,~~ challenged by China, as, ~~you know,~~ in the process of a potential invasion? And, ~~um,~~ the forecast jumped from 30% to 40%. And there were no comments under it because it was fairly new. And I was sharing this with, ~~you know,~~ one of the, ~~uh,~~ folks there that are working on from the government'pers respective. And they were saying, unless I know why this is the case, I cannot take this into account. And for that, we do pay our pro forecasters to come up with better forecasts with their explanations as well. And, ~~um,~~ we place people in jobs in the past. ~~Um,~~ so there is, in a way, the leaderard can translate into, ~~uh,~~ financial incentives. **Speaker A:**  Yeah. **Speaker B:**  Opportunities happen on the substrate of the question. And that I think is quite. **Speaker A:**  Okay, cool.

**Speaker A:** Can you say a little bit more about how you create that community forecast out of you kind of touch on this briefly, but this is one of the big questions that I've had, ~~um,~~ honestly, for a long time, because there are two kind of. I think of them as, like, canonical AI questions around AGI timelines that I've come back to for years now. I think I first tweeted about that, like, almost two years ago. And these are the weak AGI and the strong AGI. And we can unpack this a little bit more in a minute. But one thing that I've always ~~kind of~~ wondered is, ~~like,~~ exactly what am I looking at when I'm looking at this? To what degree am I seeing, ~~you know,~~ is it a naive aggregation? ~~Um,~~ because you can go in there and put a whole distribution. Right. So I'm interested in your comments on that, too. Like, I don't have a great sense of what I'm doing when I'm, like, actually drawing a curve over the potential timeline of something happening. ~~Um,~~ I feel like I'm a little bit like, ~~you know, I mean,~~ I guess I'm always ~~sort of~~ viing with these predictions, but that brings it home to me where I'm like, boy, the precision here feels like I'm leaving something behind. That is, ~~you know,~~ it's weird, right? It's a way to express my uncertainty, but it's also a way to, like, be very specific about my uncertainty. And even that I don't feel like I really have so interested in sort of how you see that kind of curve drawing. And then how are you aggregating? All these curves into one curve. Like do people get UPW weighted if they're better? Is there a recency bias, et cetera? **Speaker B:**  So, ~~um,~~ couple different parts there. ~~Um,~~ with respect to the community prediction calculation, we basically keep the most recent prediction from each forecaster that we have. ~~And, um,~~ we wait each forecast based on how recent it is before being aggregated. ~~And um,~~ we also, for the Met tacless prediction, do pay attention, especially if there is you, a paid client that is looking for a specific outcome or higher rigor. We do do things like pay attention to the past track record of each of the forecasters and aggregate that information in as well. ~~Um,~~ for binary question, the commuter prediction is basically a weighted mean, ~~um,~~ all of the individual forecasters probabilities. And if you have a numeric question or a date question, it is a weighted mixture of all of the individual forecaster distributions. ~~Um,~~ there's actually a lot of different philosophies of aggregation and this is one of the spaces that I am interested in experimenting further. These are the ones that we use. And in general, the community prediction works and seems to be really accurate. ~~Um,~~ there may be a couple of folks that seemingly know consistently beat the community prediction. And I think, ~~you know,~~ there's a couple posts, ~~um,~~ from like astral code, ext ten, et cetera, that has focused on exploring this. ~~Um,~~ I think that I'm very interested in this. Very soon we will open source metaculous. I'm curious for people to come up with, you know, hey, here's a difference scoring mechanism, an aggregation mechanism that actually seems to work better or seems to be more instrumental. ~~Um,~~ I really want to encourage experimentation on the space. What we have so far seems to work. If you just copy the community prediction, you will do pretty well in, ~~uh,~~ tournaments, but then you're not really bringing in independent information, right? So the better question to do is for someone to do their homework on, ~~you know,~~ what are past forecasts, ~~um,~~ that might be similar to this and then contribute, and only after that you see the community prediction because otherwise you start messing with the wisdom of the crowd. You said, ~~you know,~~ you might be doing vibes based reasoning. I think a lot of more. So, you know, reasoning based forecasting ends up being space for a lot of people.

**Speaker B:** Not everyone is going to build like a mathematical vigoroous model or be a domain expert, but somehow when we put many of these together, it actually does yield a better outcome. And community prediction is our way to be able to preict. **Speaker A:**  Yeah. Do you want to talk about the metaculous track record on AI forecasting in particular. I think, ~~um,~~ that's obviously a very relevant data point. And then I do want to dig into those two questions that I refer to most often a little bit more. ~~Uh,~~ but maybe the general background would be good to share first in terms of just establishing that there is some alpha in the community forecast. **Speaker B:**  ~~Mhm. Uh, what do you mean? I didn't understand. Um, there's some alth.~~ **Speaker A:**  ~~Well,~~ there's this post on just kind of breaking down the track record. ~~Um,~~ the meticulous track record for AI forecasting. ~~Um,~~ exploring meticulous's AI track record and ~~you know,~~ I basically was hoping you would, ~~uh,~~ summarize that or share the highlights as you see it. **Speaker B:**  ~~M sorry, pausing here for Min. If there's this, can you find. Let's find the specific pos if you want to zoom in on.~~ **Speaker A:**  ~~Are you in the. Um. I'll put it into the chat. I'll put two things in the chat. First, I have the.~~ **Speaker B:**  ~~There's a bunch of things that are somewhat similar to this.~~ **Speaker A:**  ~~Well, you could certainly bring in anything that you want, but here's my outline. This is what I'm just referring to as we go.~~ ~~And then second is this post. But definitely feel free to go beyond that or give whatever you know best. Summer, you think.~~ **Speaker B:**  ~~Uh, where did you, did you paste it on the end of.~~ **Speaker A:**  ~~Oh yeah, you should have. The UI here is not surpr.~~ **Speaker B:**  ~~Track record on AI forecast. Oh yeah. Yeah. This is the uh, post by Peter. Yeah, I'm gonna ask actually, can we just take a brief nature break? Um, I'll be adv.~~ **Speaker A:**  ~~Sure a sec. Yep, no problem. See you in a sec. We're back.~~ **Speaker B:**  ~~Cool. Can you hear me well, yeah, uh.~~ **Speaker A:**  ~~Yeah, I think so.~~ **Speaker B:**  I would love to go into ~~like,~~ I was just ~~like~~ looking at my ~~uh,~~ notes. ~~Um,~~ I'd love to talk a little about like interesting episodes from history we would have caught and. ~~Right. That was.~~ Maybe we talk about that a little and then we can shift to Aji questions slash d. ~~Um, the, like,~~ I just looked at the post from Peter. I can talk about that. This is very much around like, the scoring rules and scoring accuracy rather than like what are the takeaways from there? So I would address this in context of like some of the more recent things we are doing on this front, ~~like~~ the ~~um,~~ five years after, ~~uh,~~ AGI tournament. We can talk also about, ~~you know,~~ the questions that we have written on the, ~~um,~~ AGI question, like I saw on the doctor. So like maybe we can talk about those in a larger context altogether, rather than specifically dive into this post. For. I feel like the post is like a math lecture, almost like I can go into that, but I feel like it's not necessarily the interesting cy ofistry. **Speaker A:**  Yeah, ~~I mean,~~ I think my takeaway from that post was somebody had said on the Internet that the mettaculous forecast were no better than random. And it's basically a exhaustive, rigorous breakdown of why that's not in fact true. And, ~~um,~~ we can see that there is actually some amount of significant benefit to looking at ~~the,~~ the community forecast over random. But I do think that anecdotes or kind of notable examples are very much of interest as well. Please share. **Speaker B:**  So, sure. One of the things that ~~I, I mean,~~ I think it goes much more than, better than random, given that there's actually a lot of people using metaculous forecasts.

**Speaker B:** Um, I would start from, for example, the comparisons from the, ah, 2023 ACX, uh, uh, contest is quite interesting, for example, to look at, for comparison of know how does metaculous compare for prediction markets or super forecasters. And I really like Sasiti as an anchor for, I think they're actually doing a great job, ~~um,~~ as a reference point. ~~Um, this doesn't, you know,~~ I think to me, the parts of the question is ~~like,~~ how to make the forecast useful, along with how to make the forecast be accurate. ~~I think~~ there is enough track record on a couple different forums, like who predicted 2023? Best post from astral code. Extent is quite interesting for this. ~~Um,~~ but if you've been zooming on specific moments where an entity, be it a federal agency or a person, was able to take action from a forecast, I think those are the moments where I see, yes, metaculous was actually useful or interesting. Like for example, in epidemiology, metaculus has outperformed both panels of experts on Covid and also informed hospital resource allocation, public health decision making. ~~Uh,~~ a lot of the forecasts were more robust and accurate than baseline models in predicting COVID vaccine uptakes. And, ~~um,~~ I do think that'quite interesting. For example, ~~um,~~ and another part is on the monkeypox outbreakntaculous, ~~um,~~ was able to quickly provide perspective on that front. ~~Um,~~ for example, in January 2020, back when you. Conventional wisdom was that Covid wouldn't be significant. Metaculous instead was predicting that more than 100,000 people would eventually become infected with the disease. And, ~~um,~~ a lot of folks took that as, ~~you know,~~ an early warning sign. I remember reading a post about someone making a bunch of investment decisions because metaculous said so. ~~Um,~~ like in other cases around predictions around, ~~uh,~~ Russia invasion of Ukraine, ~~um,~~ there's a comment on metaulus that I really like where, ~~um,~~ the metachulous user in Ukraine said, just want to say that I moved from ~~Kyīvv Toiv~~ on February 13 entirely thanks to this prediction thread and the metaculous estimates. ~~Um,~~ like that to me is a moment where, yeah, the forecasts were instrumentally useful for someone making a decision. Like we've been, you know, ~~ced~~ in the economist, our world and data, Bloomberg, Fortune, Forbes, you name it. I think the core question is around how will these be able to turn into instrumental actions that one can take? And I'm particularly interested in this being helpful for the, ~~um,~~ AI sphere and making sure the tools that we're building, ~~um,~~ on AI as a whole are actually serving humanity. And I, ~~um,~~ think if there is something that has good track record and ability to be able to influence individual decisions, we should explore why and how this can be much more useful. ~~Um,~~ and that is the lens with which I think, ~~uh,~~ prioritizing AI is important, but also other cause areas. Some folks say, ~~you know,~~ Mettachul should purely focus on long term miss causes. I actually do think a lot of short term intervention modeling and being able to take successful action teaches us on how forecasting can be helpful in longer term contexts. Like questions like, how can we reduce homelessness in San Francisco? If I have an additional 5 million, should I just go build two more houses? Is that the best thing I can do? Or is there a form of lobbying that I do think a large crowd will say this will make the housing market understand the negative externalities of, ~~um,~~ homelessness, to the point that we can have a more well grounded change. ~~Um,~~ I would like this to be a minitaculous where every forecast, these forecasts can be conditional, they can be intertwined with each other. We can explore different ways to visualize them. Like these will bring usefulness towards someone, and us being good at this in a repeata fashion will help us be able to tackle the largest questions much better as well. **Speaker A:**  Cool. Very, ~~uh,~~ interesting.

**Speaker A:** So going to these two AI questions that I refer back to the most, I think probably many listeners will have visited these pages. The weak AGI and the strong AGI timeline. Each one has ~~kind of~~ four different resolution criteria. ~~And the question is basically, when will a single AI satisfy these different criteria? And you get to predict your distribution of dates.~~ **Speaker B:**  Right. **Speaker A:**  ~~I think~~ this has been really interesting and quite informative for a long time, although more recently it does feel like it also highlights a real challenge of writing these questions, which is that when things are farther out, it seems like the. ~~Or it can seem.~~ ~~I feel like~~ in this case, it does seem like the detailed criteria, ~~you know,~~ seemed quite reasonable. And now as we're getting closer, I'm feeling like there's ~~sort of~~ a divergence, ~~uh,~~ between what matters and what is actually the, ~~you know,~~ the letter of the law in the question, particularly when it ~~comes to~~ ~~in the week agei question, the use of~~ a specific form of the Turing test where I'm ~~kind of like,~~ okay, from my standpoint, in the sort of intuitive, like, what really matters in the Turing test line of thinking, I would say we've passed it. And yet the formulation requires an, ~~uh,~~ expert interrogation and that expert not be able to tell what is the AI and what is not the AI, and we're not that close to that. But I always emphasize ~~for people that~~ I think the reason we're not that close to that is a design decision of ~~the people that are~~ making the AI's that are to be tested. **Speaker B:**  Right. **Speaker A:**  ~~Like,~~ if I were going to try. **Speaker B:**  To pass, ~~uh,~~ how confident are you about this claim? **Speaker A:**  ~~Um,~~ I would say quite in as much as if I wanted to create an AI that would be impossible for a. Or much more difficult for a interrogator, ~~um,~~ to identify as AI or not. The first thing I would do would be have it say, I don't know, a lot more often than it does, ~~right?~~ So, ~~like,~~ the easiest way for me to tell whether something's an AI or not is just to ask ten very long tail random questions and be like, no human is going to answer these questions all ten. Like, it's just wa. ~~You know,~~ nobody has that breadth of knowledge. Actual people would say, I don't know. So I would just train, ~~you know,~~ I would actually dramatically narrow ~~the, you know,~~ the range of responses from the AI and make it seem much more conversational, make it seem much more ignorant, make it much less useful, ~~you know,~~ for what you would actually go to chat GPT for. But I think I could make it a lot harder for the expert interrogator to figure out what is what. ~~So anyway,~~ that's like just one example of this general problem. ~~Um,~~ I ~~wonder,~~ wonder how you guys are thinking about that. **Speaker B:**  So to me, the changes you mentioned, I'll start from what you just said and then go zoom back to the hi question. The changes you mentioned could probably be enacted by just using a couple language models that are policing each other, and you could basically get to a system that actually behaves this way right now. ~~Um,~~ so I'm not very sold that ~~know that~~ is the main threshold here. ~~Um,~~ there is a quote from Jren Lanier that I actually used at the beginning of my thesis that was focusing on how can we augment citizen, ~~um,~~ participation in governance, ~~uh,~~ through natural language processing. I guess today I would call itms, where he says, ~~um,~~ the Turing test cuts both ways. ~~Um,~~ if you can have a conversation with a simulated person presented by an AI program, can you tell how far you've let your sense of personh who degrade in order to make the illusion work? I think this is quite important here, ~~like,~~ being able to communicate on a certain pattern full of expectations is not what I am interested in.

**Speaker B:** I think that's a ~~very,~~ very low bar. In fact, ~~like~~ the goal, my interest goes way beyond am I able to simulate something that is convincing enough in this closed box tournament like this does not actually bring us a better world, ~~from my opinion.~~ I actually agree with you. I don't really love the way the question is operationalized, but keep in mind, they are both from 2020. ~~Um,~~ we will not always formulate questions so that they are optimally informative years later. ~~Right.~~ And I would say that in 2020, these questions ~~I you'said, it to yourself, were~~ in fact quite useful to the point that I would argue attackersus probably moved the Overton window with respect to how people were paying attention to, ~~um,~~ the impact of AI. And I think that is one value add that's both hard to track, but also intuitively resonates with me, which is what also draws me here. So I think these questions somewhat serve their purpose. ~~Now,~~ zooming out of the purpose question to ~~like,~~ okay, but can we have a better question? If we were to do this today, these questions, instead of it being a single question, I think this should be shaped something like the minitaculous instance that we were talking about, where all of the subpars of the questions would have multiple different forecasts and they all together in an ensemble, actually give you a picture of. Heck, we don't even have a clear definition of AGI to the point that the question needs to operationalize itself one way or another. ~~Right.~~ One of the, ~~uh,~~ projects that I'mt pretty excited about on metacas is to come up with indexes, basically come up with just the way you can ~~aggreate~~ a bunch of stock tickers to come up with a composite view. I would like there to be 30 forecasts that are all really zooming in on slightly different aspects of this. And them all together is what you're paying attention to now. You can say, well, I want to rigorously link all of these through causal diagrams, and then you will get into a whole different hair ball. Sure, that could be helpful if you pull it off, but even a lesser fidelity version of this ~~that is just~~ here is 30 of forecasts that all rhyme with each other with respect to its focus point will be able to shed more light. And I think at that point, with what metaculous had, like that was possible. And these questions did serve their purpose. ~~Um, like,~~ it's a challenge of forecasting the future. ~~Right.~~ We don't always know what formulation for a question will be most useful years from now. Now, ~~um,~~ for example, we're doing the five years after AGI, ~~um,~~ question series that just launched. ~~Like,~~ I am much more interested in that, for example, because when I look at the answers there, it actually gives me a much more accurate view of what people even conceive of as AGI. ~~Like,~~ the way I am using those questions isn't about thinking of, okay, ~~uh,~~ will these things happen when AGI hits it more so informs me, okay, these are the things that people consider as critical possibilities with an AGI or not. And just doing a throwback to some of my prior work with AI objectives institute, I often find there to be, if we are talking about AGI and AI alignment without talking about the societal context in which this has impact, I think we are narrowing this down. ~~Like,~~ for example, there's a common meme that we would use in AI objectives institute to talk about what successful AGI that can enable human coordination is in the world. That is post AGI. If you don't think Jerusalem will be a united and peaceful city, maybe your AGI isn't ambitious enough. ~~Like,~~ maybe something that is truly AGI would be able to resolve clashes and cruxes within existing human sphere, where it's not trained to make sure it is perfectly neutral towards that, but instead it augments human agency that actually brings a meaningful shape.

**Speaker B:** Because otherwise, you know, sure, I canh up a system so it can say, I don't know, enough time so that you might find that convincing. I find that to be a lower. I think we should be more ambitious. **Speaker A:**  Yeah. So ~~what do you,~~ how much editorial do you exercise over these questions? Because it does seem like these couple of questions have ~~sort of~~ become a bit of a shelling point. And it's a delicate, ~~um,~~ decision probably from your perspective, right? Do we ~~sort of~~ take a proactive step to retire a shilling point because we feel like it's outlived, ~~uh,~~ its usefulness or do we just let the community ~~kind of~~ gradually move on? **Speaker B:**  I wouldn't say that at all. I think like these questions are good. I am much more interested in pushing it further. So ~~uh,~~ I guess it's also ~~uh,~~ a call to action slash ask for help. For everyone that loves metaculus is interested in this. As we launch minitaculus, which are focused instances, like if you have different formulations that you think are interested, come tell us, ~~like,~~ we already get a lot of inbound questions, right, that are around by Ky. Can we write a question? We heavily editorialize them for we do want to maintain a level of rigor. And so far I think metaculous has been much more close than what I would like it to be. I would love for there to be many more people. That's right, questions, especially domain experts, especially people who say, hey, I get the hang of what a question is resolvable and meaningful and these are things we can help with. But we are entering a chapter of metaculous as we are open sourcing to get many more people to write questions, to have instances that they are hosting that have maybe different scoring mechanisms, ~~you know,~~ different levels of rigor to the point that it actually proves useful to them. We will also host ataculous.com, many instances that are very domain specific. So I just think we need more questions. It's not about you going back and changing what this has accomplished, but more so bringing on new lights. Like for example, I'm quite excited about a bunch of new technologies that could actually bring better results. ~~Like I love a lot of the, or, you know,~~ symbolic base. Like are we able to use language models in order to ~~um,~~ come up with specification criteria and ~~um,~~ use that to create safeguard AI systems? ~~Uh,~~ I'm interested in singular learning theory. There's a bunch of new techniques that actually, when I imagine versions of AGI or you, weak levels of AI competency, even that is really good at these, the ways in which I envision those features are substantially different. And I am ~~quite,~~ quite interested in exploring these. And I think that just requires a, ~~ah,~~ breadth of more questions to forecast and also more human energy or AI energy to be able to forecast on these questions. So anyone that is interested in saying, hey, I have things I want to forecast, I have a formulation that I think is better, come talk to us. **Speaker A:**  How do you create density though, right? ~~I mean,~~ the one worry that I would have if I was you and I got a million new questions in, is now I need ~~like, you know,~~ a billion predictions, ~~right,~~ to actually create a meaningful community forecast for all those million questions. So how do you think about balancing the diversity of questions versus the density of forecasts? **Speaker B:**  Right. Great question. **Speaker A:**  Um, this is also where the AI's can come in. We'll get there momentarily. **Speaker B:**  Yeah, the nail on the head, like the real currency here, the real limitation is human attention bandwidth. Right. I have 10,000 questions being launched at metaculous every day. It's not going to help anyone, at least until everyone is forecasting part time as part of their job. That's not what I'm trying to angle towards here.

**Speaker B:** But this is why we need to do active research on questions like, can we build indexes that aggregate a bunch of different forecasts? Can we have AI start forecasting as part of, ~~uh,~~ uh, AI benchmark tournament? We've had an explosion of AI contributors. They seem to be doing okay. And can that create a scaffolding or jumping off point for humans to be able to get to a rigorous forecast much faster? ~~Um,~~ there is, I think, a lot we can do to increase the quality. ~~Um,~~ the wind condition isn't, we have 10,000 new forecasts every day. I think that just will drown any kind of quality from the system. And ~~the wind condition also isn't,~~ well, AI's are forecasting and humans are just watching. I think the question is more around are we able to identify what are the best questions? And that requires, ~~I want~~ that to happen with more people, not just you, the metaculous team. And I want the contribution of the bot to be able to gear towards shedding more light and more information and have these questions be incrementally composable, ~~um,~~ so that we can build world models of, ~~you know,~~ all of these are rhyming with respect to how we are thinking about electric vehicle proliferation, for example, ~~you know. Um,~~ and using those, then we can have much vaguer questions like, are we able to, ~~you know,~~ like forecasting specific windows of electric vehicle proliferation in China is quite specific. But if I have 30 of those questions, I can then make something aggregate that says, ~~you know,~~ will electric vehicles be better for the environment? It's ~~like,~~ what does that even mean? But ~~like, it actually means~~ a compost of all of these different things. This is a different frame of reference for thinking about how we can aggregate them. **Speaker A:**  Cool. ~~Um, well~~ that's great motivation then to get into ~~kind of~~ what the state of the art is in LLM powered forecasting, and then the tournament that you're running to try to advance that state of the art a little bit further. In preparation for this, ~~uh,~~ I read through three links to papers that you had shared, and I was overall pretty impressed. You can give me more detail on kind what you think is most important or stands out, or what the kernels are that you really want to build on. But it seemed like, across the board, pretty positive results. And these are from serious, ~~um,~~ authors, including Tetlock, has been involved in some of this research. Just to summarize the three papers, the first one was approaching human level forecasting with language models. And this was from, ~~uh,~~ Jacob Steinhardt and his group. They basically created, I guess, what I would describe as ~~sort of~~ the intuitive thing that I would create if I was going to work hard on trying to make this work. And that is like a retrieval system, the ability to go on the Internet, search through the latest news process that, ~~ah, um.~~ And ultimately create forecasts. And it seemed like it was pretty good. It was like coming close to the community forecast, although not as good as the overall community forecast. ~~Um,~~ a question that I did have reading that paper, which I don't know if you would know the answer to, is, okay, it was a little bit short of the community forecast, but, ~~like,~~ how does that compare? If it were an individual human, you what, at what percentile would, ~~um,~~ that system have performed? ~~Uh, I don't know if you do know that, but's~~ my intuition was that it would be pretty high as ~~like,~~ the percentile of individual humans. **Speaker B:**  Right. I mean, let me give you a thing that I do believe in, that I don't have data, but I think we will soon have data for this. Is that part, in the hands of a team of human forecasters will just kill it. Like, that is obviously going to be much better. Right. Going back to the question of usefulness, I like the academic rigor aspect, don't get me wrong.

**Speaker B:** I do think testing these in isolation is good. And a lot of our designs around, ~~uh, um,~~ AI benchmarking for, ~~uh,~~ competition pays attention to that. But the part that already is obvious to me is let's actually look at what these systems are good at. They're good at going through massive amounts of content, identifying which parts may be relevant. They're good at taking a first step at, ~~um,~~ creating a world model. ~~Um,~~ let's actually build systems that pay attention to those first, ~~um,~~ and build on top of that. ~~Um,~~ I particularly like the sch. Steinhardt lab. The Halloween paper for a lot of intuitions that I would like to explore further is in that paper. And a lot of folks that have asked me, I want to compete in the tournament. What should I do? I always point them, ~~well,~~ start from here and then try different things. Try different ensemble methods, use different language models. ~~Um,~~ share prior data, don't share prior data, have them debate each other, have a third model, look at it and synthesize them. ~~Um,~~ there's a lot of playful strategies that one can go with. So I really enjoy thinking in this framework. ~~Um,~~ so I've been encouraging, ~~um,~~ there's a very active discord, by the way, for the, ~~uh,~~ tournament where people are discussing strategies. It was absolutely delightful to see how much the models have evolved in just like the three weeks of the tournament kicking off on, ~~um,~~ different strategies people have come up with. And we do encourage people to update their models. ~~Um,~~ for the tournament, there shouldn't be human in the loop because we'trying to benchmark the state of affairs. But if they have a better strategy, they should update that. ~~Um,~~ another thing we do is reasoning transparency. Have the models post at least some form of text we don't grade or score this because it is a whole different level of complexity. For that, let's stay with the forecasting accuracy. But that at least gives us a way in which how the model is thinking. And a lot of earlier research on chain of thought reasoning has pointed that, ~~you know,~~ explanation is actually what gets these models to be able to stay further grounded. ~~So,~~ to me, these things are really good. ~~Um,~~ couple other avenues that I don't think ~~has~~ been explored as much lately is how can we bring in model based reasoning on top of language models that will actually yield much, much better results? Even in the first few weeks, we have, ~~what,~~ 15 questions that have resolved so far? I think, ~~um,~~ in that signal, we've had plenty of questions that have, for example, tests like scope sensitivity or, ~~uh,~~ negation. ~~Um,~~ ask the same forecast, ask the opposite. See if the answers are the opposite of each other. ~~Uh,~~ we've had examples like, for example, there was the measles question. ~~Like,~~ if you ask the bots, ~~you know,~~ what will be the number of know measles cases? Less than, ~~uh,~~ 200. It'll say 70%, 75%, 70% for more than 300. And that means, ~~you know,~~ there should be five chance that it will be between 203 hundred, and bots will say, ~~you know,~~ 65% for the window that's between 203 hundred. ~~Like, obviously,~~ a pro forecaster wouldn't make this mistake. But even an ensemble method falls apart on this case because it doesn't keep track of. Here is a world model that is mathematically rigorous. Now, why is this interesting to me? Humans wouldn't also be able to do a very good job at tracking this if they were much more complex. ~~Like, obviously~~ a pro forecaster is trained to get better at it, but if you ask this to someone that is, ~~you know,~~ not highly numerous child, they would be like, oh, maybe it's the same. Oh, I didn't realize that this is wrong. That means there's something intuitive about how humans reason that is distinct from the mathematical rigor here. How can we close that gap? ~~Like,~~ if we are able to incorporate model based reasoning?



































