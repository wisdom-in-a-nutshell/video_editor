**Speaker A:**  Jud Rosenblatt and Mike Vianaa, CEO and R and D director on the alignment team at AE studio. Welcome to the cognitive revolution. **Speaker B:**  Thanks for having us. **Speaker A:**  You guys have built a remarkable company with a, uh, fascinating backstory and some really interesting recent work, so we've got a lot to cover. As you know, regular listeners know, I very seldom ask the backstory question. I think yours is quite different. So I'd love to give you the chance to tell the backstory of how you started AE studio, what the mission is, and how you've developed the company to what it is today. **Speaker B:**  The backstory of AE is basically back in 2016, I realized that I had the luxury of living off my wife's salary and never having to work again. That freed me up to think more deeply about the long term future of technology and humanity. One thing I thought a lot about is that I sometimes like to think long, complicated thoughts that take a while to convert into words. By the time I convert them into words, I might say something entirely different from what I imagined in my head. Humans aren't very good at communicating to their future selves, let alone other people, compared to what they might be with better tools of thought. I considered that there might be a low probability that within my lifetime, humanities's primary mode of interaction with computers might come to be via brained computer interfaces. If that does come about, one, it would be very good to try to make it come about in a way that's more agency increasing for the end user instead of manipulating people to do things they don't want to do and interrupting them mid thought with ads, having Mark Zuckerberg own the extensions of their thoughts. And two, it might be a way to make humans sufficiently smart and wise enough to be better able to solve some of humanity's greatest challenges, including particularly solving AI alignment. And I continue to think there is a possibility we need substantial human augmentation to solve the alignment problem. From there, I decided to create a profitable bootstrapped software and AI product development consulting business, building products for clients without taking any money from investors, ~~uh,~~ with the thesis that we like to build products that increase. What we talk about is human agency, with the idea being that it's better business sense in the long run to increase your end user's agency because that makes users like you more, want to use you more and spend more money and refer their friends to do the same. Taking that same philosophy with the work we do with clients and making great value for them and having them want more work and refer their friends, led us to grow the consulting business to over 160 people. We took the profits of the consulting business to invest in building our own agency, increasing startups. So we built and sold the first one of those and plan to incubate quite a lot more in the future as well. We take the capital that we produce from these businesses and invest them originally in trying to build maximally agency, increasing BCI software without profit motive to help people with neurological injury and disease increase their agency, but also in the future make the nature of human existence substantially better and make us smart enough to solve the alignment problem. The consulting business grew. We sold the first startup, we made a name for ourselves in the BCI world and worked with top BCI hardware companies. Super excited about the work we did there, pushing people to build things in more agency increasing ways than they otherwise would have done. A few years ago, after I had kids, I was paying attention to what was happening with AI, because we build stuff with AI ourselves all day and noticing that timelines might be shrinking faster than I would have hoped. We have this unique structure where we are unlike all the other for profit businesses that have anything to do with with AI alignment.

**Speaker B:** We are not incentivized to create AGI ourselves. We don't have any investors, we don't have to have returns to them, and we instead can do the right thing long term for humanity and consciousness in general. We also are entrepreneurial and good at getting stuff done and getting to the root of the problem, and we have to be, because we're paid to do that. For clients, we have this a guarantee which is that we will treat your client project as though it is our own startup, as though we are founders of your own startup. Everyone is constantly priming themselves with and reminding themselves of that every day. Which means that the top idea on their mind is often while they're taking a shower or walking the dog, thinking about how if I were a founder the startup, how would I make it super successful? And it's great. That isn't just the case for ~~claimient~~ projects, but also for the work that we do with BCI, which allowed us to have a real impact there. So we thought without profit motive just trying to solve the alignment problem, maybe we should start to put the profits of the company into alignment. And we didn't know how to do that at first. We spent a lot of time thinking about and researching how to have the biggest positive impact there, and eventually realized that with the effective altruism ITN framework, the highest impact thing to do is actually things which are individually, unlikely to work, but very high impact if they do work. Neglected approaches. And so the BCI thing we were going to start with was a neglected approach, and it worked better, faster than we would have guessed. But still, the timelines would be much longer for sure. We concluded that we should work on neglected approaches ourselves. One of the neglected approaches we realized is potentially impactful is encouraging others to work on neglected approaches too. But that's mostly not what people are doing historically, and we're excited to see people work on more and more neglectiate approaches to do them ourselves internally, which we're working on, and excited to talk about the specific ones that we've worked on and shared publicly so far. Also excited to encourage the alignment community to work on more and more neglected approaches and realize that individuals can go ahead and start a startup which everyone else thinks is going to fail. You can also work on a neglected approach to alignment that no one else believes, and it might just work. **Speaker A:**  I love it. There's an unbelievable amount there that I want to dig into. I guess for starters, at a high level, this plan sounds ~~kind of~~ crazy. If I heard this at a startup pitch competition, or if somebody sent me, ~~uh,~~ a two pager with the plan that you have executed already, I would say there's way too many steps in this plan. It's going to be really hard to bootstrap a consulting business and be profitable enough to do these other things, and you're going to have the tug of clients pulling you in one direction. So, ~~uh,~~ what would you say are the keys to the success that you've had in building a business? Having these lofty priorities, executing on the day to day, but actually keeping those in mind and staying agile enough to actually change. We're not talking about incremental changes to your strategy here, but pretty significant fundamental changes to the strategy. I would say. Maybe you have some tricks or some special attributes that put the odds more in your favor, but I would love to know what they are. **Speaker B:**  I'm not sure entirely. I can guess, and probably it's something to do with what I might guess, but I do think that it's an unconventional path. And because it is so unconventional, the assumption is that it's not going to work.

**Speaker B:** There are all sorts of things that are unlikely to work, and also all sorts of things that are likely to work, but the things that haven't been done before, people are more likely to assume are not going to work. I remember when I was first starting the company and outline the plan to people, everyone was like that's crazy. Of course that's not going to work. And I think that a lot of the reason why it has has been that it just sort of makes sense. It's a de risked, conservative approach to doing fairly ambitious things, and I think we're doing it for the right reasons. It was impactful to realize that I wasn't motivated by money and I could live off my wife's salary for the rest of my life, and then realized that I could think very long term about the future of technology and humanity. And if you do the right thing every step of the way, then it winds up yielding better results for you as best you can over time. There's this really interesting Paul Graham essay called Mean People Fail, which puts forth the idea that we are moving from a world in which you've got more resources by taking resources from other people, like conquering neighboring lands and stealing their serfs. ~~Uh,~~ and we're moving from that world into a world in which you get more resources by creating value for others. It's better to do the thing that creates value for others and be a good person to work with, such that people want to work with you in the future and provide more opportunities. And the best startup founders he knows are very nice, good people. That seems to be objectively the case. However, a lot of the things about the way the world works are still rooted in older ways of doing things. This is why people build products and service companies that optimize for short term revenue over doing the right thing by their users, and increasing value created for users and profits for the company. For example, the company that we built and sold was a direct to consumer subscription e commerce SMS management tool. Subscription Eco commerce is a big growing thing. People get subscription coffee, protein bars and whatever else. The perennial problem is once you signed up for one of these things, eventually you have too much of the thing. What we built allowed you to manage your order via SMS. You never get any promotional marketing stuff pushing anything on you. It just made it so that from when you first signed up, it made it super easy to change your order or skip a month or whatever. And what this wound up doing is substantially reducing short term revenue for the companies that used it, but also substantially increasing lifetime value of customers. It's interesting just how obvious this is, but people are inclined to leave a lot of value on the table because they're not inclined to do things like this. So when we were first doing sales for the startup, potential customers would respond one of two ways. They would either say, oh, this is awesome. Of course I want to sign up for this and have my users get the right amount of the thing that I'm making for them and passionate about putting out into the world, or they would be like, I don't know about that. I don't want my revenue numbers to decrease. That's not going to look good to my investors. It's a very stark difference which of the two approaches you would get. And honestly, if we were a lot bigger, instead of being acquired, I would have acquired the company that acquired us and then used that litmus test to evaluate whether or not to invest in these direct to consumer subscription e commmmerce companies. This is sort of core to our DNA, has been fairly good for us.

**Speaker B:** And incidentally, also for people thinking about starting alignment driven organizations, nonprofits or for profits, I think if they start out mission driven and don't compromise on that, and only bring along partners who are in fact going to to be aligned with, uh, that it makes it a lot easier to do things all way along the way. And it does lead to the creation of bigger opportunities in the long run as well. The company came after the core values themselves, and they do allow for a lot of this stuff. We're very big about starting small, but then taking acceleratingly large a b tested baby steps that allows us to go into new things that we have no business doing. Because we can assume we're going to be terrible at at first, but with the growth mindset, we'll get better and better over time. The self efficacy beliefs that ensue from having done that a little bit, you'll be the ability to do substantially more and more ambitious things. I think people tend to overestimate what they can do over a short period of time and underestimate what they can do over a much longer period of time. So we set out to do very ambitious things. We set originally like this ten year, big, hairy, audacious goal with a goal about the consulting business and the skunk works companies and the BCI stuff. And we wound up actually achieving that whole thing way faster, way sooner than we could have guessed. Then we made that all more ambitious. We thought it was unlikely to happen, but we could do one or two of these things. Ideally, if we can do the third thing, that would be great. Let's first perfect the consultings, then start to do the works, then start to do the BCI stuff. And now the agiign and stuff. Interestingly, in the very beginning, I set out to do all of them simultaneously. BCI skunkirks and consulting. I did a terrible job of all of it. ~~Uh,~~ it was okay enough, but it didn't work. Then we went back over time and perfected, did ~~oneed~~ one really well, and then set that off the run quite effectively with the right external structures, that things work way better. And I think we're also quite interested in building things that are sort of like in a cyborgism type way and extension of oneself as a result of the things we've built internally. I am probably a hundred times more productive in a single day than I was when I started the company. It's great to be constantly thinking about stuff like that as a skill, an organization, and investing. We have a lot of the best developers and data scientists in the world, and we're always investing in things to build and use for ourselves to improve the quality of our lives and increase our agency. **Speaker A:**  Okay, I'm gonna have to be disciplined in my follow up questions, ~~but I have~~ long term thinking has come back in fashion. You're a great, ~~you know,~~ example of setting out with that mindset. Hiring people with a mission driven orientation is commonly received advice, but easier said than done, especially if you're sort of like, our mission is to solve the AI alignment problem, but your job is to build this SMS tool for subscription management, free e commerce. ~~That's where I see a lot of these things.~~ Not even necessarily with goals as ambitious as addressing AI alignment, but even just, ~~uh,~~ on a more conventional level. Oh, we'll do technology consulting and then we'll make our own products and apps, and we'll ~~kind of also be a startup~~ that typically doesn't seem to work. People ~~kind of~~ just get consumed with the consulting. People have to do a good job on these projects, ~~right.~~ For the whole thing to go. How do you, from like a leadership standpoint ~~or~~ a culture standpoint, how do you get people focused in on the sort of foundational tasks that may be the less sexy tasks that everything else depends on in a way that keeps people happy. **Speaker B:**  Yeah, absolutely.

**Speaker B:** To be clear, I don't think they are less sexy myself. They're interesting technical problems to be solved at every level. The brightest, most ambitious growth mindset people in the world can get excited about whatever that might be. It is because I myself will genuinely get excited about whatever those things are. On the consulting side, I genuinely am excited and care deeply about every single one of our client projects and think they're awesome and want to do everything I can to make them succeed. We've scaled the organization in such a way that I can from the more ~~remotd~~ perspective I'm at have a positive impact on them, but also trust really great people to be fully focused on them. But originally I was fully focused on them and that was all I did. Although actually originally I did all sorts of other stuff at the very beginning because we're a profitable bootstrap business. I also rented our office on Airbnb. I remember m my partner on the first day he started, a dutch tourist stepped out in a towel and asked him, where is the bathroom? He was completely shocked. In any case, you might be the brightest machine learning engineer in the world, and you might be the person who thinks of the thing that ultimately solves alignment. Uh, but you still have to do the dishes sometimes, and you might as well enjoy yourself while you do that. The things that we do are way more exciting than doing the dishes. They are legitimately exciting in and of themselves. The better job you do with it, the better job you wind up being able to do with everything else. Because we're a, ~~uh,~~ growing company with a lot of opportunity, doing more and more ambitious things over time. There is unlimited opportunity for individuals in the long run to grow into various different things. Basically, if you are what we call a good tripper, you do the right thing and take on more than your own fair share and you're willing to push yourself when things seem difficult. This comes from me having led wilderness trips for many summers in Wisconsin, in Ontario, where you take a group of kids who would be sort of, not necessarily beforehand, any experience doing difficult wilderness trips, and then by the end of the trip, we would engineer an atmosphere in which it was cool to be a good tripper, to do more than your own fair share, to carry the heavy pack, get into camp and realize that tents need to be set up, vegetables need to be chopped, and water needs to be pur ifified. Didn't do that without being asked. Push yourself to carry a canoe farther than you think you can when you thought you couldn't even pick it up. There's no better feeling than throwing the canoe off your shoulders into the next lake after doing that. It's interesting because in life today, people often coddle themselves way too much and then they don't wind up getting the super rewarding feeling of throwing the canoe off their shoulders into the next lake. But if you do push yourself in various different ways, then you can wind up achieving bigger and more rewarding. And I think people who wind up being good trippers on whatever they wind up doing and creating the maximal value they can for our clients or in the skunk works company or whatever it is, then they can also, they're the people are going to go on to have the biggest positive impact and the various other things that we do as well, and have a bigger positive impact on the world. Incidentally, to one fun little hack that we set up, actually, my wife, who's our CTO and far smarter than I am, set this up. We donate 5% of our profits to effective altruism charities. So whatever. And typically there are things that are shorter term, not longer term things. So I tend to push for them to be longer term things, but like malaria nets and whatnot.

**Speaker B:** That means people know that every hour you work on some annoying bug, uh, that's a build hour to a client, you wind up actually donating 5% of the profits of AE to save lives as effectively as people know how to do, which is intrinsically motivating, I think. And that's been pretty awesome. And I think we have a lot of other little things that add up to make it such that people can feel motivated on a day to day basis. But the reality is sometimes there will be things that are quite difficult and challenging and not motivating in whatever work you might be doing, whether it's working on alignment or some client project, or whatever other job you might do. I do think that people starting nonprofits and for profits doing AI safety can have the mission of doing something for AI alignment. That itself is very motivating, important, and potentially quite urgent. It would be great, for instance, to nerd snipe a lot of the brightest startup founders into making their startup is have something marginally, ~~uh,~~ to do with increasing probability of AI alignment instead of just advancing capabilities. It would be great to get successful founders who are trying to figure out, okay, now I've made money, but what's the meaning of life? To do something that's impactful for the world and advance AI safety at its core. And there is a lot of low hanging fruit there. ~~Um,~~ or if they're ideally not motivated by money after having an exit, they could go work on AI safety. There's a real, ~~uh,~~ scarcity of people who just get stuff. There are a lot of great people who get stuff done, but there are not enough people in general working in this field. And it would be great to have more people who, ~~uh,~~ can execute, ~~uh,~~ run organizations and work on more neglected approaches to AI safety. **Speaker A:**  That's all fascinating. 100 x more productive position today, as opposed to when you started. I often say, depending on the exact task, ~~um,~~ I think a multiple more productive with AI tools at my disposable today, but I'm definitely not 100 x more productive. So I want some of that. Give me a little flavor for how you have achieved that. **Speaker B:**  Yeah, absolutely. There is quite a lot of different stuff that we do at the company that I do on an individual level, but I would say probably the single thing, which alone has made me 100 times more productive because it's made me at least a hundred times more prolific. The things that are able to get out of my head now go to execute in various ways. The company is a specific tool that we built, which is sort of a launcher. It's called universal launcher. It's really mostly customized for people at our company, though others can't use it. You can download it on universalauncher.com dot. But basically, the idea is like, there are limits to your working memory. Previously, I thought I would be doing something, and I think, oh, I should do x, or, I want to ask my assistant to schedule a meeting. Then I might have to open up slack and message them and be like, would you mind scheduling a meeting with this person at this time? Or I open a Trello card to have us discuss x, Y, and Z, and I have to open my web browser and open a new tab. Type in Trello dot coma, wait for it to load, select the right board, and go to the right column. Basically, as a result of the limits of working memory, there's an increased probability that you're going to lose the full context of whatever you thought of originally when you want to get it out of your head. This, I think, personally, is one of the greatest injustices of modern humanity as it exists today. I wouldn't be surprised if future generations look back on us today and be like, wow, those people could never think complete thoughts.

**Speaker B:** And that's, like a terrible travesty up there with other great inhuman things that had to exist. But in any case, it also can just make you a lot more productive with a single command. I press command spacebar on my Mac or on my phone. There's this, like, little button there, the you button, and I can push the. Tap that, and it goes straight to the launcher. Then I can add to do items to my own to do list or for other people or to different boards to talk about x with our CFO or ask my assistant to do y. Interestingly, I use this, as do most other people at our company. It's remarkable how much more productive it winds up making you. But also particularly like one thing that's pretty exciting about it is that you don't have to use universal launcher to have this effect. You just have to capture the ideas. But the fact that I know I can capture the ideas and I can trust the external structure that they will wind up getting executed with high probability of success, because historically they have been, that means that I'm way more likely to capture way more ideas than I otherwise would. That's pretty liberating, and it winds up making me think of much more interesting new ideas, initiatives and solutions to various different things. I can be reading some random paper and then think, this is really cool. I should have us look into this particular neglected approach and then capture that, and it actually winds up getting done and pretty amazing and awesome. I think the optimal end user experience of an individual, like using technology eventually will be something m closer to that, but also having AI agents wind up implementing various different things that you'd want done more fully. And we've built some parts of that into this tool as well. I think the thing that has made me the most productive is the fact that I can get more ideas out faster. Instead of holding in my head, I have to do x. When it's no longer held in my head, I can be fully focused on whatever the other thing is I'm doing. And I think a lot of the time people lose agency. You might be working on some super complex technical problem, but then you see a missed call from your cousin and you're like, oh, I should call that person back. Then for the rest of the day you might be multitasking between, oh, I got to call that person back and trying to hold all this random complex stuff in your head. So you're not going to hold it in your head as well, and you're not going to solve the problem as well. The solution you come up with is going to be substantially worse than you otherwise might have. So just being freed out to think more deeply in a way where you're totally present in what you're doing, but also you can trust that whatever other distractions come up get put into the right external structures to deal with, I think is been quite liberating for me. And then we've tried to build structures like that throughout the company for other individuals to be able to capitalize, and the same thing like Mike, what is your experience with universal launcher, for example? **Speaker C:**  Yeah, ~~um,~~ basically what you're saying. I think one of the things that's, ~~uh,~~ interesting about is just all of the integration. So Judd was talking about Slack and Trello, but there's lots and lots of integration. So it's just a matter of idea to muscle memory to capture. So I think that's on a more concrete level, going from idea to just muscle memory and now it's captured and you can release that idea and move on is actually a huge productivity game. **Speaker B:**  What I think is cool. **Speaker A:**  Universalauer.com I'm on the website right now. **Speaker B:**  Mike, you worked at a for years before you ever used universal launch. What changed? How is life different as a result? **Speaker C:**  What changed to make me want to use it? **Speaker B:**  Sure.

**Speaker C:** **Speaker C:**  Is that the question? I working previously, I can't imagine not using it anymore. It feels like an extension of myself in a way, in that sense that, like I said, it's muscle memory to do it. I haven't thought about trying to measure if it's 100 x productive, but yeah, going through a UI to try and capture an idea and then, ~~um,~~ all of the distraction and how long that takes versus just capturing it immediately. I had an update and I had broken something with it at one point with the install, and so it wasn't running. I felt like I couldn't use my computer correctly, like my muscle memory wasn't working and I had to relearn how to do it the old way temporarily. So, yeah, it's really a powerful thing. **Speaker B:**  There is a steep learning curve that. **Speaker A:**  Does start to feel like the. **Speaker C:**  That's true. I think one of the reasons I didn't use it for many years is because there is a bit of a learning curve. It's a little bit like learning an instrument. So, yeah, if you're willing to go through that learning curve to payoff is huge. **Speaker B:**  BCI one thing that's very cool about universal launcher, I think, is that this is just one example of all of a thing that substantially increases our agency with technology and makes me human better. We're on technology all day, day in and day out, and this thing fundamentally solves the thing. Like, you take out your phone to do x and then you see a bunch of push notifications, and then you're like, why did I pick up my phone in the first place anyway? And instead we take out our phone and push the one button, go straight to do the thing and don't have to deal with the push notifications. We don't want to do it. I think that this sort of thing, though, it's less obviously something that solves alignment. These things are going to be increasingly important in the future as we have AI taking over more and more things. It's gonna be important that we understand the way the human brain works and then build end user experiences that increase human agency and improve the nature of being human. There's low hanging fruit to build stuff like this. But just as nobody sets out to build profitable bootstrap businesses with big visions and long term missions, people don't set out to build technology products that have steep learning curves and no clear plan to get revenue models, but no care for that whatsoever, just because they want to use the thing to increase their agency. And as software gets cheaper and cheaper to build, it's going to be pretty great that people can invest increasingly in things that actually probably substantially improve the nature of being. Universal launcher makes my life experience way, way better. I can't tell you how much better it is. I so much more enjoy be e as a result of this. And I think that this is just the tip of the iceberg in terms of what people can create. And also the more stuff like this that people do create, the more productive and happier they will be to better solve the alignment problem and other problems to people. Could also do startups to do things that increase agency and make alignment researchers happier and more productive. **Speaker A:**  The way you guys talk about the muscle memory involved does naturally lead into, at least for me, thinking about brain computer interface. We now have neuralink out there with an implant in a couple of individual'brains and the way they describe their experience is, you know, very much the sort of muscle memory type experience where they're learning to use this interface and they have to develop a new skill, essentially a form of motor skill, even though it's all mediated by that implant. How did you think about the brain computer interface problem?

**Speaker A:** Do you think about it similarly to how Elan has talked about it, where he talks about increasing the bandwidth between the computers and going along for the ride with the AI era, uh, or br a different foundational way of thinking about it. I'd love to hear a little bit about some of the most interesting projects that you've done in that space over the last few years. **Speaker B:**  Yeah, absolutely. The original motivating factor with regards to alignment and BCI was less being able to keep up with AI and more making humans smart enough to solve the alignment problem in the first place. I think there is a possibility we might be able to keep up with AI via BCI, but I don't think it's a very high probability eventually, at least. So it seems somewhat wishful thinking, but at, ~~uh,~~ the very least, we'll be able to go faster and think better, and that'll be pretty nice. But I do think there's a lot of opportunity to just make us smarter to be able to solve the alignment problem in the first place. In a world where we get basically unlimited funding going towards alignment, which I do think is actually entirely plausible, everyone suffers from exponential slope blindness, and that does not predict the future particularly well when it comes to things with exponential slopes. So, ~~uh,~~ we might wind up finding ourselves in the low single digit number of years. In a world in which there is effectively unlimited funding for alignment, and nobody knows where to put it, the world might start to say, ~~you know,~~ we spend trillions of dollars globally annually on defense. We should spend at least that much on the biggest threat humanities has ever faced. The people who could do meaningful things to advance lman are sorely unprepared for a situation like that. ~~Uh,~~ BCI in particular is one area in which you can invest a great deal of money and potentially have ~~huge,~~ huge impact when that does come about. ~~So,~~ especially if you get truly automated science that doesn't kill us, and not just automated AI, but also various other automated things in labs and Tesla robots. ~~And that's entirely possible.~~ That comes about decades of BCI progress in the space of a year or a month. ~~Of course there are limitations, because~~ requires human patience generally, but there may be ways around that in terms of the biggest innovations that need to be made. It's ~~a~~ very much a neglected approach in a larger portfolio of bets, which works best in that scenario where you get automated science that doesn't kill us. And I think that's most promising. But then there are other things that work there too. Mar and others around KE are quite excited, and I am ~~too~~ invested in the human intelligence augmentation, gene editing for human intelligence augmentation startup, which could make a big impact. But specifically with BCI, the original thinking was that we wanted to, without profit motive, build open source BCI operating system, Linux style, which would enable the development of multimodal BCI systems that would increase human agency and human flourishing. We started with Orig, originally a guy who now is the CEO of an fro doing ultrasound was an AE client doing machine learning. He helped us kick off our BCI initiative when we had no idea what we were doing then our original forays were sort of embarrassing, but we figured out what we were doing with him and got involved working with top labs around the world, doing various new ML methods to accelerate BCI development. ~~Uh,~~ a motivating factor was that Facebook ~~met~~ and Google were really involved in developing BCI, and we didn't like the idea of them owning the extension of our thoughts, as I said before. So without profit motive, trying to build maximally agency increasing BCI, ~~uh,~~ one thing that started to put us on the map is that we participated in this prestigious neural latednt benchmark challenge, which was started by, ~~uh,~~ Chathan's lab at Georgia Tech.

**Speaker B:** And we use state of the art neural models combined with various advanced ML techniques to win that challenge. We worked with labs doing advanced VCI like Braingate, which had, ~~uh,~~ been at the forefront of BCI systems, and, ~~um,~~ then also focused on ~~sort of~~ finding the tools and methods that can be shared across different labs to accelerate their research, and collaborated very closely with them to develop a bunch of different interesting projects, different open source projects, and got some grants to do so as well. Those things include the neural data simulator, which worked with an awesome collaborator named Chad Boul and enables BCI development without a participant in the loop. Another one is the ne NDK, the neurotech development kit, which we did with, ~~uh,~~ guy name Quentin, and another guy named Miln, who are both doing very cool things in this space now, too. New neuro data standards for things like NWB, which is Neurod data without Borders, brain imaging data structure to facilitate better data sharing across groups, and privacy preserving ML methods for neural data to improve user agency privacy, doing a bunch of different blockchain stuff, and distributed learning systems, which we applied to BCI using techniques like federated learning and homomorphic encryption. And basically, then after doing all this for the community, we also started working with leading BCI companies. We worked with BlackRock Neurotech, which makes the Utah array the first, ~~uh,~~ FDA approved implanted invasive electrodes with forest Neurotech, some, ~~uh,~~ our former chief scientist at AE on their minimally invasive ultrasound systems that let you get 1000 x more data out of the brain than you could previously do, and also supported work and are excited about work being done by various other BCI adjacent startups like, like Journey, the Jana Meditation Company, Stroke DX, and others as well. Basically, a lot of our work is neuro inspired. We want to do even more neuro inspired work, and we still have a lot of great talent in neuroscience and BCI on our team. We are focused on doing whatever the highest impact neglected approaches are. So we're doing increasingly things that are not, ~~uh,~~ BCI related necessarily in the short term, but also ~~various~~ neuro inspired, neglected approaches. I think there are lot of great neuro inspired neglected approaches that can be done. **Speaker C:**  ~~No, I think you covered.~~ ~~I'm happy to supply more details. Nathan, anywhere that you would want to.~~ **Speaker A:**  Dig in, ~~maybe~~ let's ~~kind of just, uh,~~ zoom out and ~~give, like,~~ the high level view. Where are we in neurotech or brain computer interface right now? Where do you think we're headed over the course of how many years? Why do you think that is? Too long to wait and ultimately forced you to. I don't know if you would call it a pivot or refocusing, but you've changed your strategy. So interested on, you know, kind of your vision, and you, what made you think that vision is just not one that you can afford to be patient enough to see through? **Speaker C:**  Yeah, if you think about BCI, there's lots of hurdles towards making progress. You see things. But even with neural link, which is maybe state of there, Blackrock, who we work with, state of the art in terms of invasive BCI and high bandwidth methods, there's still some fun. Even just getting FDA approval, it's a very slow, ~~uh,~~ iteration cycle, going from struggling to decode our movements or getting cursors to work, keyboard decoding or something like that, we're not there yet. So the timelines from going from. From that to increasing human intelligence to a point to help solve alignment, for example, or even the ~~bcnos,~~ and having BCIB an extension of human thought, that timeline is much longer. And so if you think about AI alignment and AI safety and what timelines look like there, and we want to do the most impactful thing, it's not that BCI isn't a worthwhile pursuit. And of course, we're still doing neo science inspired approaches, and we still have BCI work.

**Speaker C:** But I think the fork there is clear that if you want to do the most impactful thing, then AI alignment, for us, especially taking some of our background and applying it there, this neglected approach is just, we can have a bigger impact there in a shorter amount of time than that very long term. Audacs go for BCI. **Speaker A:**  Just to be concrete, what do you think that timeline do you have a sense for when, for example, a neuralink experience and perhaps not a neuralink device, but a sort of. I can augment myself with some new thing that allows me to interact with computers purely through my thoughts, and in a way that kind of becomes second nature. How far off do you think that is from being something that is like a consumer device? That is a commonplace because it's super useful. **Speaker B:** ~~I mean,~~ I think that's a very difficult thing to answer because of the exponential slope blindness thing. Any response that is too soon would make anyone in neurotech think people with neuroscience backgrounds that be like, oh, my God, you're crazy. But, you know, Elon Musk and others have outperformed expectations a lot in the past, and AI is certainly accelerating everything a great deal. But even still, with that said, we did conclude that the thinking that set us down this path of doing more other neglected approaches was something like, hey, let's say potentially for BCI to get to the point that we'd really like it do to certainly substantially accelerate AI alignment, ~~uh,~~ that's maybe at least 30 years out. That was what we discussed internally, and we said, okay, let's do other stuff that can be higher impact over a shorter period of time. With that said, ~~uh,~~ we still want to push that forward as much as we possibly can. And I think it's good, if you can afford to do so, have a portfolio of bets over various different timelines. People spend too much time thinking like, oh, we can't do this because of this timeline, or that because of that timeline. But it is worthwhile for people to work for a prolonged period of time on an approach which will, like, realistically, your timelines will change over time, most likely. And people's timelines wind up updating way too much on whenever the recent news is. And just as people update way too much on the probability of one presidential candidate winning or the other based on the most recent news. The unfortunate thing about approaches that are over longer timelines is that people are more likely to abandon them as. Because they start to think about shorter timelines. So I do think it is quite important that people with longer timelines or really good ideas for things that would require longer timelines continue to work on those neglected approaches, and that funding is allocated to those, like, people think about the most impactful work that would take decades, and then actually, like, go set out and do that, and think about the multiple big, ambitious steps that need to be taken, and then execute them one by one. And also focus on the biggest unknowns that you have to de risk that are going to take the longest period of time if you can afford to do so. **Speaker A:**  So, if I understand the narrative history, you had the neglected approaches idea for a long time. The brain computer for interface work was one neglected approach. You're climbing the mountain, getting over your initial embarrassing, uh, failure, starting projects, actually getting to the point where you're working with real leaders in the field, winning prizes yourselves, making meaningful contribution, and then I think this is pretty recent. My sense is, like, in the last one to two years sort of realized like, geez, this stuff is happening fast. Maybe we don't have time to climb this full mountain. And then from there we need to go resurvey the landscape and figure out what currently are the neglected approaches and, and which ones do we feel like we can tackle.

**Speaker A:** And then from there, you, you actually went out and did a survey of the field to try to get a systematic, uh, handle on that. Tell me about the survey and what you learned from trying to figure out is common and what is neglected today? **Speaker B:**  Absolutely. That timeline is generally accurate. ~~Um,~~ with the survey specifically wound up happening to confirm a lot of the punches about neglected approach stuff that we had, that we didn't set out to do that and had no idea what to expect. But basically, ~~like,~~ the survey sort of came about as a validation to a meta approach of trying to figure out the highest impact things to do. And basically, ~~like,~~ we learned that alignment researchers don't think current AI safety research is on track to solve alignment in time. They also don't think that current approaches cover the space of plausible research agendas necessary to solve ~~aignment.~~ And yet, what do people wind up putting the most time and resources into ~~the things that are,~~ these things that are not likely to solve alignment, ~~like,~~ you know, two big things being mechanistic interpretability and evals, which of course, ~~like~~ we think is very good to do that work and quite important. And there're also neglected things within both of those things. And we're working with Goodf Fire AI, for example, as a client doing an awesome mechanistic interpretability work there. But, and we're very glad that they're doing that. ~~Um,~~ interestingly, even though people realize that, and that in particular is ~~like a particularly like~~ neglected thing that they set out to do, I think the Herald a, like a new age of people building awesome AI safety ~~proviven~~ startups that are really mission proven first and foremost. With the general research being done across the board, people realize that we should do other neglected approaches. In theory, it would be worthwhile to do that, but on an individual level, it's good for your career or you're more likely to get funding to do something where you can make more progress. There is a lot of ~~vot haing~~ fruit there. So people just think that we should do something. And I think that made more sense in a world where you have unlimited timelines and it's more of a green field like the way EA used to be. But the reality is when there is a possibility of short timelines, then you just ~~sort of~~ have to do whatever it takes to make the thing work. The survey results indicating that people don't think we're on track to solve alignment in time and the space of current approaches does not cover all. ~~Plausible~~ research agendas needed to solve alignment seem to indicate that we should spend way more attention and resources pursuing neglected approaches to alignment, in addition to any other practical steps that make alignment research more likely to solve alignment. ~~Uh,~~ before we get AGI, I think it's great to see that people are realizing this more and more, using this term more and more. Most alignment researchers don't actually view AI safety and capabilities advancement as mutually exclusive, but they assume that other people will, which is unfortunate. There's often a disconnect between what individuals think and what they think everyone else in the community thinks. That makes them afraid to pursue various approaches. When we first started to get into alignment a year and a half ago, we encountered this pushback about advancing capabilities. People are often scared to take any action or even think about doing something ambitious to solve the alignment problem for fear of advancing capabilities generally. One thing that most people realize, at least on some unconscious level today, is that capabilities are moving forward super fast, regardless of what people who care about AI Safy area are doing. ~~Um,~~ I would prefer to live in a world in which we get BCI right before we invent all this stuff and where we understand what the heck consciousness is before ~~weinvent~~ all this stuff, but we don't. The full economic forces of the entire world are behind advancing AI.

**Speaker B:** Uh, capabilities right now is as fast as they can. And I think, ~~like,~~ we have to recognize that that's happening. Therefore, we should try to solve the alignment problem. Ideally, they don't do it too much, but there's also interesting work out there around doing things which are sort of like a negative alignment tax. I think that's a pretty exciting thing too, like doing things that increase the probability of actually solving alignment while also potentially, ~~like,~~ you advance capabilities a little bit. The assumption though, is that anything you do that makes, ~~uh,~~ AI more aligned is going to wind up reducing its capabilities. But we haven't necessarily seen that historically. What if we live in a world where it turns out that you sometimes advance where it's possible to advance alignment and then be more capable by virtue of your alignment? ~~Uh,~~ that would be preferable. If that is the case. Maybe it's not the case, maybe it is. ~~Uh,~~ we shouldn't over index on that being the case, but if it is, that would be great. It would be great to ~~def~~ find ~~and do more negot~~ ##ed approaches that support and explore that possibility. Regardless, the fact that alignment that AI capabilities are off to the raises like the marginal impact of some people working on AI safety, doing a little bit to advance ~~uh,~~ capabilities as orders of magnitude less than would otherwise be done, you might as well do stuff that winds up advancing alignment if you can, while still being responsible and having good safety plans and things like that. ~~And yeah, so I can also, I guess~~ other things that are interesting from the survey are that, ~~uh,~~ both the effect of altruism, community and alignment, communities overestimate the perceived value of intelligence in the community and underestimate the value of softer skills like collaboration and work ethic. As soon as anyone here is, they're like, oh yeah, of course that's the case. ~~Um,~~ but I think they're pretty big implic ##ations there in terms of if you want to be more effective and have a bigger impact. Historically, a lot of people who might be super needed in various organizations for advancing AI alignment were turned away because they weren't math geniuses. But people with these software skills are very much needed to make organizations run, ~~uh,~~ more healthily and succeed better. I think there's a lot of need for that and low hanging fruit there. **Speaker A:**  But you're saying that the community as a whole recognizes that they're needed, but still individuals within the community don't think that that recognition has happened. And so people presumably with those skills are sitting thinking, well, they won't want me because I'm not a math PhD. **Speaker B:**  Yes, exactly. It would be great if more people realized that, because those people are enormously needed and often those sorts of people who can help grow organizations. Imagine the idea that we might wind up having unlimited funding in a low single digit number of years. But if we do, I think it's essential that every alignment organization has a good plan for what to do with that and how to scale in a healthy way that doesn't grow too big too fast and do the opposite of what they should be doing in those situations where that does wind up happening. And I do think there is some chance that happens, we should be prepared for it. You'd want to have way more people with these softer skills to help grow these organizations in healthy ways. **Speaker A:**  Yeah, that's a great tidbit people should definitely make note of. We'll make sure to clip that and put that on Twitter. So let's get into the couple, ~~uh,~~ recent papers that you guys have put out, both of which I think are super interesting that are your current line of attack, your current neglected approach to AI alignment. And then at the end we can zoom out a little bit and talk meta and you can offer some thoughts for others who might want to develop their own neglected approaches.

**Speaker A:** The neglected approach that you guys have taken, you can give it the proper title, but the way I understand it is biological inspiration. Not just at the level of looking at a neuron or a circuit, but zooming out and looking at bigger phenomena in biological systems and thinking like what if we ported those over to machine learning systems? What would that look like? And I think if these first two results are any indication, this is a super, super promising line of inquiry. The two are self modeling and self other distinction minimization, and we'unpack both of those in detail. I don't know if you want to give any sort of high level motivation. There's the background in brain computer interface, so that puts you in position to think along these lines. Uh, but then I'm really excited to get into the real details of both of these because they both jumped off the paper to me as like, wow, ~~uh,~~ more people should be doing this. Why haven't I seen this before? It seems like versions of this have a home in the systems that I expect to predominate in the future. So I was really impressed with the work. Give me a little more motivation for it if you want, or we can dive into self modeling. **Speaker B:**  I think that motivation you described is, ~~uh,~~ basically accurate. One important point to add is that we are taking this larger meta neglected approaches approach'trying, to find neglected approaches as best we can and encourage them as best we can and do whatever the highest impact wants to do are. And one part of that also happens to be finding individuals with really great ideas and hunches for things who are not sufficiently supported, and making their dreams, things which are individually unlikely to work with very high impact if they do a, ~~uh,~~ reality. We are good at evaluating those sorts of things and making them more likely to happen, because this is what our client work does. In our client work, the client is the product owner, and then we are the technical team implementing all the technical work, but also we're managing it and trying to figure out the highest impact stuff to do to actually make the vision come to reality and be ~~uh, a, uh,~~ business and technical success. The client is not accountable for managing or implementing the technical details themselves. That turns out to lend itself very well to finding and working, ~~uh,~~ with a good track record of knowing when clients come to us. I have a pretty good sense of how successful they're going to be from doing this for many years now. And I think that there's a fairly similar thing for finding individuals with neglected approaches and evaluating them and then setting them up for success to do that. So in this case we did do those two particular things. I do think it's interesting that this is a model that's a very neglected approach which we would like to see scale substantially, build out better ourselves, scale it more, and then get as many other organizations around the world as possible to copy it. Because if we wind up ~~leivinging~~ in this world in which there is unlimited alignment funding, then you want to solve the talent gap as much as you can, getting impactful people to work on alignment solutions. And I think ~~uh,~~ Ryan Kidd and Matt have this great less wrong post about the three different types of alignment researchers. There are people who are going to be more likely to come up with really good alignment ideas, but they don't necessarily have the capacity to fully implement the whole thing. And I think that in an ideal world, we would be setting up perfect external structures to maximize the output of the most brilliant creative people thinking of neglected approaches to AI alignment, and then also have really great structures and teams set up to just immediately go to the races and support them and make that thing happen. And like have effectively unlimited grants. Making that happen, unlimited grants.

**Speaker B:** And eventually there'll be good ideas for this that are for profit companies as well. If you start from a place where there is unlimited funding and think about how we solved alignment, given unlimited funding, then you want to optimize the creation of these ideas and the random, like unemployed autistic people with crazy ideas who say something about them on the Internet and no one listens to them and they have the social skills to sufficiently communicate. That might just be one of the essential elements in solving alignment. We'd like to systematically find those things and then build the best possible, ~~uh,~~ implementation for it and see if it works. In an ideal world, right now we only put the profits of our company into the alignment work that we do. ~~Uh,~~ haven't taken external funding. That's core to how we think about doing things to retain agency. We're not opposed to taking grants in the future, but basically ~~like uh,~~ what we'd like to do in an ideal world is have this skill and do way, way more work like this. And then you can have the most talented, competent software consulting organizations in the world copy us and do this instead of building other random crap. They should just, ~~uh,~~ not to say the other random stuff is not valuable. It's awesome. But, ~~like,~~ as many people as possible could be working on actual meaningful alignment work. And, ~~uh,~~ a thesis we had was that senior AI engineers with no previous background in alignments could quickly get up to speed and do very meaningful, impactful work in alignment. And we've proven that certainly is the case. And in fact, if anything, maybe they're more productive than your average alignment researcher because they do technical work all day instead of just reading less wrong in slack groups all day. No offense to all the people who do that. I'm glad you figure things out by doing that, too. In any case, ~~like,~~ it's a thing that can scale way, way more, I think. And it's worth that eventually happening in various different worlds. That's another thing people can do. And should you start consulting organizations, there's consulting organizations to do other people's alignment. That's one easy way to bootstrap, ~~uh,~~ a thing into being a bigger impactful alignment, ~~uh,~~ organization in the future as well, and eventually developed its own ideas. But I think that's a really good thing. But in any case, with both the self modeling and, ~~um,~~ the self other overlap stuff, there were two really individuals who had these neglected approaches, and we've made them both happen. The self modeling one was an awesome professor at Princeton named Michael Graziano. And our work in self modeling is inspired by his mechanistic theory of consciousness, which we can also get into. It's pretty interesting and awesome, but there are two or three really interesting high level things in terms of motivation and what our results are pointing to so far. First, self modeling may be the architectural change that results in conscious AI, ~~uh,~~ which also validates Michael Graziana's mechanistic theory of consciousness in some cool ways. Second, we think that self modeling is a strong prior for cooperation, so it may also be an avenue to bias AI systems towards cooperation. Third, we found that self modeling induces a simplification of the network. This has potentially profound implications for neuroscience, psychology and machine learning. And for example, we've been working with, ~~uh,~~ John Barge, who did some literature review and reached out to colleagues in his field and has the opinion that there has been no former connection between self modeling and simplification in psychology literature. So we've stumbled upon, ~~uh,~~ something pretty major here. And then also from a pure machine learning point of view, our implementation is actually pretty simple and elegant, and we can get into the details, too. But for now, basically, this means that it's trivial to add to pretty much any neural network. Both from implementation and training costs, ~~uh,~~ perspective, it's not that expensive.

**Speaker B:** Even though we still have a lot to understand about self modeling, it may turn out to be a state of the art training technique, which would have profound implications, since we would be teaching the model to do self reflection and provide a neuro plausible mechanism for consciousness in these artificial systems. **Speaker A:**  Let's go one level deeper. You guys can divide it up how you want. Often I try to give the setup, but maybe in this case, Mike, you should give the, ~~uh,~~ research setup. This is. That's very high level, lofty stuff. ~~Um,~~ but it does translate to a very practical, and I found, for the most part, quite intuitive experimental structure that is, ~~like,~~ honestly not that hard for people to wrap their minds around. And I think that is, ~~like,~~ part of the magic of this paper that attracts me to it. So give us the low level details of what you set up and what you found. **Speaker C:**  Yeah, sure. Happy to. The setup is, as Jud said, ~~uh,~~ extremely simple, elegant. An undergrad could implement it and test it. So. And that's part of the appeal. We can start at the low level. It's also nice. We'll eventually connect it back to the neuroscience and where it was inspired from and the motivation more directly. At a low level, what we do is train a, ~~uh,~~ neural network and add one extra output layer for your technical audience members. You have the activations of the penultimate layer. They typically go to an output class. ##ific layer, a linear layer that actually tells you what to do, ~~uh,~~ if you're doing a classification task. And so we just route those activations to another layer that says, predict these incoming activations. You could choose different targets in the neural network, but we chose, ~~uh,~~ for the paper. The one that we're reporting is the one where you, ~~uh,~~ model those incoming activations. And so you just add another loss function that says, do your task, but also predict yourself. That's the entire setup. And then we measure what happens when we train like that. Maybe we could get into the details. Do you want to talk about the setup? ~~Or I can flow into what the results are, too.~~ **Speaker A:**  M. Yeah, I think that makes sense. Our audience is pretty technical, and we don't shy away from those details. Most people will be very familiar with concepts of activations and what a neuron is. And it's a theme, for sure, that clever modifications to a loss function, ~~uh,~~ can bring us great benefits. So this is another instance of that where it is remarkable. There's now two elements to the loss function. **Speaker B:**  Right. **Speaker A:**  I think they're just literally added together. ~~Right. So you've got.~~ You've still got to predict your job, as the neural network is still to predict the number, and you're using MNIst, like the classic handwritten digits. So you still got to do your job. But now you have this additional term, which is you also have to predict what was the state of the layer before, essentially the middle layer, because ~~this is a, you know,~~ pretty small system that you're going in such depth on. And now you've got to do both. And it turns out you can do both. But interesting things start to happen when you do so. **Speaker C:**  You know, in some sense, you give the network the task, predict your internals, and so what can it do? It can learn to predict those well, but it can also make. The internal is easier to predict because it has control over that. It's intuitive that the learning process would result in simpler internals. We said that might happen. And then the thing to test is, if you end up with simpler internals, can you still perform the task, like just self modeling, just derail the training process or not? And it turns out it doesn't. So we don't lose any performance, but what we see is self modeling. You get simplification across a couple of different metrics. What you find is it's still very good at doing the task.

**Speaker C:** This is Mcar and IMDb classification. So, across all three tasks, three different models, two modalities, you see the same trend that when you turn up self modeling loss, you get simplification, and you don't lose any performance. So that's the major result. I think the interesting thing here is how it was motivated and how it ties back to the neuroscience aspect, which is it's motivated by this theory of attention schema. So, our collaborator, that judge was mentioning, Michael Graziano from Princeton, is a neuroscience consciousness researcher, and he has a theory of attention, a mechanistic theory, and it's based on self modeling. And, uh, people have different theories of consciousness, but it's intuitive, and it makes predictions about what should happen. That's where the motivation comes in, based on self modeling in the brain. What if we do this very simple, allgant version of self modeling, and then we see simplification and interesting connections back to neuroscience? **Speaker B:**  Yeah, we might as well go in. **Speaker A:**  All right, we've done one recent episode on, ~~kind of,~~ broadly speaking, possibility of consciousness and AI systems with ~~kind of~~ a rundown. When I get to the attention schema, ~~uh,~~ and other similar things. There's information to integration theory, which is another one I d put in the same bucket of, ~~um,~~ fairly materialist mechanistic proposals. I find them to be interesting, yet maybe I just haven't spent enough time with it. I can't quite make the leap where I'm like, how does this solve the hard problem? This might make mechanistic sense. I can much more readily tie it to behavior. That's probably all we could really say about ~~the, you know, the system that you guys have trained~~ or that, ~~you know,~~ the. The models that ~~you guys~~ have trained. ~~I am interested in.~~ ~~If you have a formulation of how that leap gets taken to how I feel. Things like that I'm not quite getting over on my own. I'm definitely interested in your thoughts on that. And then I do want to get into the complexity metrics as well, because that was probably the part of the paper for me that was the hardest to understand.~~ **Speaker C:** ~~Do you want to say something about ASD in general jed and I can support?~~ **Speaker B:** ~~Sounds good. Yah.~~ **Speaker A:** ~~Going strong. And I'm back. So as soon as I see myself get off of zero. There we go. Okay, cool. I guess I'm interested in the attention schema, uh, particularly interested in how we make the leap to subjective experience there, or why we think this is more than a mechanism and actually the source of consciousness. And then really interested in developing my own intuition for the complexity metric that you used. And sort of what do we understand about the simplification that we're seeing as a result of this self modeling mechanism?~~ **Speaker B:** One thing that's pretty interesting here is that the self modeling task biases the internals of the network to become more modelable or predictable. Our results show that these networks, while simpler and more parameter efficient, don't lose performance. So, like, keep that in mind as we talk about attention schema theory. Attention schema theory, for some reason, is hard to get through your head. Once you do deeply get it through your head, sometimes your brain just sort of rejects it afterwards. Or'like, oh, I totally get it now. And you're like, this makes sense, and completely explains consciousness. Then the next day you're like, wait, no, that doesn't make sense. It doesn't explain the quality of stuff. For some reason, this winds up being the case, which is unfortunate and makes this be, unfortunately, a neglected approach which people are not funding anywhere near enough. If anyone's listening to this and wants to send some funding to Graziano's lab, they would very much benefit from it. Even though Gratiano is a brilliant researcher, with all these past successes, people don't like funding his consciousness research because it's too mechanistic and not magical enough.

**Speaker B:** But even if you believe in magical theories of consciousness and not mechanistic theories of consciousness and you want some magic to be there, you still might as well understand reality. You can layer in your pretend magical stuff later on. The reality is we don't understand. Even with attention schem theory enough about consciousness at all, you need to make a lot of progress to better understand the reality. If you want to better understand things and you have other theories, it would be worthwhile to make way more progress. There's so much low looking in FR of awesome stuff to do that we see right here. And Graziano does, too. Basically, like, the fundamental idea of attention schema theory is that it has proven useful and made accurate predictions about what would happen in artificial agents, which seem to be enormously impactful for alignment, which, at the end of the day, is the main thing that I care about, rather than getting lost in things that we don't sufficiently understand scientifically and people want to debate forever, might as well just be concrete and progress. Attention schema theory suggest that in the same way we have a simplified model of our arms and legs, we also have a simple, imperfect model for our attention, a model that includes how we take and process all sensory inputs with are in our brain and what our goals are, whatever we're paying attention to. So, like body schema theory, if you've ever seen those videos where there's a guy with a fake arm in front of him and someone hits it with a hammer and his real arm on the side suddenly shoots up from under the curtain, or if you're familiar with phantom limb syndrome, these are examples of where the body schema, your body's imperfect model of your body, conflicts with your actual body. It's important to note you can't ever actually know reality. All you can know is your imperfect models of reality. That's the reality. You have an imperfect model in your brain of that cheeseburger. Similarly, you also have ~~an, uh,~~ imperfect model of your own attention and attention schema theory. Again, like, whatever you're paying attention to sensory inputs and introspection and whatever, in attention schema three, this model of consciousness, ~~uh,~~ in any complex system, it's useful to have a controller with a simplified model. The cool thing about this model of attention is that it's potentially very useful in regards to alignment, because the same model of attention that we use for ourselves is also applied to others. So you have an imperfect model of your own attention. That is, your consciousness. Imagine that your consciousness is your imperfect model of your own attention. You also have an imperfect model of my attention and, ~~uh,~~ of your dog over there sitting on the ground's attention. And what is my consciousness? Well, my consciousness is also an imperfect model of mind, which means that when you experience my imperfect model as my own attention, it is as though you're sort of experiencing my consciousness. I mean, you're not really. But in using the same mechanism in your brain, ~~uh,~~ you are effectively experiencing it as though it is my consciousness. And I can also only experience an imperfect model of, ~~uh,~~ my attention. People are often better judges of themselves in the third person, for instance. So someone might have a better model of your attention than you do yourself. It's potentially just empathy at a very deep level, and then encourages more proc social interactions. So what we try to do is replicate that in an artificial model, and that's all the background you need to understand the motivations and what's going on here. You could go deeper into attention schema theory, ~~uh,~~ but it's worthwhile to have that quick background and then go into the technical details, because you can dispute the theory of consciousness as much as you want. But the cool thing is, it did accurately predict what winds up happening in artificial agents, which is some validation of something to do with it.

**Speaker B:** So even if you say there's some more other stuff to consciousness that doesn't totally cover it, still, it's useful for this itself. I think it's a shame that people don't sufficiently understand at all what consciousness is, and we might be about to have it emerge, or maybe it already has, for all we know, because we don't even know what it is in larger models. And I think it's very important to try to better understand it at a much smaller level, because then you substantially decrease one existential risk, potentially. Two, you substantly decrease moral patienthood concerns because you better understand it at a smaller level first. ~~Uh,~~ rather than having it, ~~uh,~~ be way worse at a much bigger level, potentially, you also better understand what you need to understand to find out. For all we know, maybe lms are already conscious in some form and we have no idea, because we don't know what consciousness is. The better we understand it on a smaller level, the better we'll be able to figure that out. And maybe they are, maybe they're not, and there's no valence to it. We don't know. But those are things that are worth finding out. Still, our focus is on solving alignment in particular. And if that happens to move forward consciousness research, that would be good. We do think the neglected approach of real scientific mechanistic approach is to consciousness research is essential and really important to do in the first place to better understand what consciousness is, just better self alignment. Our focus here is, and the specific part of this where it's like focusing on solving the alignment stuff, not on specifically, just understanding what consciousness is. Mike, is there anything more you want to say on that? **Speaker C:**  Yeah, that's a, ~~uh,~~ pretty good overview of a lot of things. I wanted to distill some of that and bring it back to this actual, the self modeling work, and, ~~um,~~ pull out some of those key points jad was mentioning. A very quick summary of what we were talking about on Ast is that your brain ~~createsel~~ self models. So you would have a model of your arm. And how do you know it's a model? Because it can come out of sync with your actual arm, like phantom lamb. Or these experiments designed to ~~mess, uh,~~ with your perception that actually your model adopts a rubber arm as its own, for example. So there's experiments that show that. So it's not too hard to accept that your brain creates models of itself, let's say, of your arm that you have cognitive access to. And these models typically co vary with the real thing. Like my model of my arm, I know where my arm is in space. I have access to it, and it's coariing with my actual arm, typically. And then for attention schema theory, what co varies with your attention? Well, it's your subjective awareness, right? ~~Uh,~~ that typically what you're cognitively aware of is co varing with what you're paying attention to. And then a guy in a Barar suit walks past and you ask people, did you see this? And everyone says no. Perceptually, they saw it. It came into their sensory system, but they weren't paying attention to it. Your cognitive subjective awareness, what you are aware of is covariing your attention. That's, ~~uh, uh,~~ a quick summary of ast and why. The model, this subjective awareness, this model of your attention, ~~it~~ it is consciousness. It co varies with your attention. That's the theory. And it's important to have this model because that's where self modeling comes in, and that's where we bring it back to our work. Another piece of what Judd was talking about is this model that we have ourselves to predict. Our own attention can be reused to predict others attention when we think about how we might cooperate. Part of that is at least understanding what each other are. Paying Attention to.

**Speaker C:** So maybe you've seen some theory of mind tasks where even toddlers, somebod reaching for a cup, but they can't quite grab it, but the toddler can. And so the toddler is paying attention. I know what they're thinking. I'm predicting what they're paying attention to. It's that cup, and they want it, right? So then they go and hand the person that cup. So the connection between this model of ourselves and predicting other people ties this self modeling work, actually, to cooperation and theory of mind. So that's part of the motivation for how we connect all of this back to cooperation and alignment. **Speaker A:**  It makes intuitive sense to me that I need a model of my arm because my brain is going to coordinate what's going on with my body. It's a little less intuitive why I need a separate model from my attention. ~~Like,~~ why do we need this sort of multi layer thing as opposed to just having one system that does the paying attention and deciding what to do? One possible answer for that would be the social thing that you said. But it seems like the account given in the theory starts with modeling the self, and then is that we're reusing it for others. I don't know if there's enough wiggle room or uncertainty in the theory that it could start with modeling others and then get applied back to the self. I'm interested, ~~uh,~~ if you can go a little bit deeper on that. You mentioned that there's, ~~you know,~~ there has been this sort of usefulness in terms of, ~~like,~~ making a prediction. ~~Is,~~ does the theory predict that we ourselves, ~~like,~~ become easier to model because we are self modeling? Is that like the prediction that then got repurposed over to the machine learning side, or want to just kind of make sure I have a, ~~uh,~~ very sharp understanding of what the prediction was that was validated. **Speaker C:**  Yeah. The result of the paper is the surprising part in terms of, from a machine learning perspective, we are going, we're predicting. I think our already mentioned that it's going to simplify the internals, but then understanding the connection between, when you add self modeling to a neural network and it simplifies the internals, that's surprising. From the neuroscience perspective, there hasn't been a connection between the self models and simplification of your internals, for example. So that's not what the prediction is. There's actually other work that shows that when you enable agents in, ~~uh,~~ multi agent environment ##ime with an AST like controller, that they're on a cooperative task that they perform better. So that the theory of ast says, when you have this ast and you can predict each other and you have this mechanism, that you'll be more cooperative. So in that sense, the framing of ast is helpful to make predictions about cooperation there. So just disentangling those two things. **Speaker A:**  Yeah, just, I guess it seems like the. Maybe you sort of answered it just now, but I was just trying to understand, ~~like,~~ why do I need to model my own attention? I understand why I need to model other people's attention or other, ~~you know,~~ things, attentions. And so I was wondering, like, ~~is the,~~ is the theory specifically opinionated on self modeling coming first? ~~You know,~~ because, ~~uh,~~ I think you guys said ~~like,~~ that we're reusing our ability to model ourselves, to model others. And I was wondering, is ~~like,~~ is that order baked into the theory, or could it be that we needed to model others and then ~~sort of~~ repurpose that to model ourselves? **Speaker C:**  Yeah, ~~uh,~~ my understanding of it is that, ~~uh,~~ that is the proposed ordering, but I don't know that the theory of ast requires that, or even necessarily has particularly strong opinions on why we have self models. What benefits do they have? For example, our paper has this unexpected benefit of simplification, potentially, that the self model imposes. So that could be a why that the theory wouldn't have necessarily predicted.

**Speaker C:** ~~Uh,~~ I think it's more that it's an observation that we do have such a model. Not only there's more that we could dive into on ASD and why, ~~uh,~~ the arguments for the theory itself. ~~Uh,~~ but I think mostly the theory says it's an automatic thing. You can't choose to turn it on or off. You can't choose to create it or not create it. People have this conscious experience, which within this framing is a model of our own attention. And so, ~~yeah,~~ it's not necessarily a theory of how we develop such a model, but just that it is there and that it coari with attention. **Speaker A:**  Incidentally, let's talk about complexity. **Speaker B:** ~~I, uh,~~ was just going to say it's worth noting that latency, a perfect model of attention, would be computationally expensive and unnecessarily detailed. So there is benefit to having this simplified model, and then it's more easily manipulated by other cognitive processes and lets you do quicker decision making and predictions about, ~~uh,~~ attentional states. So, ~~like,~~ it's useful for that alone. And then as a result of that, there is this subjective experience of consciousness by providing this simplified, reportable version of, ~~uh,~~ our attentional state. ~~So'I think that probably explains I might not sufficiently emotionally satisfy you about that question, but I do think that~~ explain it. **Speaker A:** Gotcha. Okay, so, in terms of the simplification that we see when we actually apply this in machine learning, you guys have a metric, which I honestly don't have a great intuition for. So I'd love to get a better intuition for. How do we define complexity? How are we know, observing it go down? How much is it going down? What does that mean? And do we have any sense for, ~~like,~~ what the upshot of that actually is? ~~Like,~~ it sounds great, but is there something we can do now that we couldn't do before? So, yeah, everything on the complexity metric and meaning. **Speaker C:** This is, I think, an important point, which is we wanted to show in the paper this result that you turn on self modeling and you see simplification. But we're actually not that interested in simplification or reduce complexity for its own sake. What we're interested in, ~~uh,~~ from following the theory of ASD, is this idea that this mechanism of self modeling can be useful for cooperation. It makes you more predictable. So the simplification is a proxy for the idea that if you turn self modeling on, you actually become more predictable to others. That can lay the foundation for cooperation, potentially. But we don't have a predictability result yet, or in the paper, we're not showing a predictability result. It's something we're actually looking more actively into. Predictability itself is a form of simplification. If I'm easier to predict, I'm in some ways simpler just at being more predictable. So the simplification is a result in that direction. We likely won't use those same metrics in future work. For example, ##le the RLCT, which we can talk about the real log canonical threshold as a metric that we use. ~~Uh,~~ it's computationally expensive to approximate and won't really scale to LLMs, for example, or at least not current, ~~uh,~~ state of the art in terms of how you approximate that. And ~~so, uh, yeah,~~ that's not a metric that we would plan to use in much larger models, for example. ~~Um,~~ but we would like to still. We would like to anchor on this idea of predictability. So it's not important necessarily for anyone to understand or work to understand that particular metric. I think it's m more important to understand this broader picture of self modeling simplifies in some way, and, in fact, it actually makes these things more predictable. So that's one clarification. And then, ~~um,~~ in terms of the actual RLCt metric. ~~So,~~ so it comes from singular learning theory. I'm not an expert in singular learning theory. It has, ~~uh,~~ some of its roots in algebraic geometry, but I can give an intuitive picture of what it might be measuring.

**Speaker C:** So if you think about the idea of a lost landscape and imagine that you're on a flat road and around you is mountains, and so in this picture, you actually can drive straight and not change your loss. So you're in a local minimum inside this valley. So what that means if you think about the loss landscape and your weights, you actually have a single dimension where your weights aren't impacting your loss at all. So you have one too many extra weights, maybe not literally a specific weight, but some configuration of the weights, a dimension of freedom to move a along the ~~lost~~ landscape. So that gives you the sense already maybe, that the geometry of the ~~lost~~ landscape, this fact that you have this one dimensional flat minima, is telling you something about your effective number of parameters. And so singular learning theory actually shows that these over ~~parameriized~~ networks not only look like this one dimensional mountain view that I'm describing, but actually a crossroads where you have this loss, this minima crossing over itself. So you think about these lines, that crossing point is actually a singularity, like a self loop. There could be cus there could be all sorts of singularities, or not just could be, but r there are singularities in the ~~lost~~ landscape for neural networks. And so, ~~uh,~~ when you're at the singularity, you actually have two dimensions you can move in and ~~not, not, uh,~~ change the loss, right? So now you've lost kind of two effective parameters, so to speak. ~~Uh,~~ because you have two full dimensions, you can move in without changing the loss. And so that's some intuition about why the geometry of the ~~lost~~ landscape might be related to the effective number of parameters. And then RLCT is trying to measure that local geometry and actually measure the actual effective number parameters in your neural network. So lower number, lower RLCT means lower effective number of parameters being used to solve the tasks. So therefore simpler, really interesting. **Speaker A:**  I mean, when I think about trying to apply this to larger systems, it seems like there is a real question here about whether we need to be working with over parameterized for sort of phenomenon to happen, or the trend obviously, in language models, is toward overtraining. And it seems like we're sort of for inference, ~~uh,~~ cost reasons now well into a regime where it's like keep the parameters relatively small, jam as much stuff in there as we can possibly stuff into it, who cares if the training process might be a little less efficient at the end, we're really going to spend a lot of the compute on inference. So fine, but would that potentially create problems for this simplification phenomenon to happen if you were to try to apply this technique to a larger scale, not. **Speaker C:**  From, in terms of the actual ~~loss~~ landscape in singularity, it's more about a computation of approximating the RLCT in terms of just measuring it if we wanted to continue to measure it in different systems. But yeah, the theory shows that given that there are symmetries in neural networks, you're always going to have singularities in your ~~lost~~ landscape. In fact, singular learning theory, again, not an expert, ~~uh,~~ but is saying ~~like, hey,~~ actually not only neural networks, like hidden Markov models and mixture models, lots of classes of learning algorithms have this feature about them, which is the ~~lost~~ landscape is highly singular. And so we really need to understand learning theory from this perspective, recognizing that ~~there's~~ these singularities and seeing where that other assumptions break down and accounting for those, and updating the theory to that. So in terms of cramming more data into a smaller model, it's still not a problem. You would still have singularities in ~~lost~~ landscape, so you would still expect to see and be able to measure something like that. **Speaker A:**  Yeah, I need to do an SLT episode, I think.

**Speaker A:** I guess my intuition is still, this is not a strong intuition, but I'm just imagining, ~~like,~~ there's so much work going into distilling models right into the smallest possible form. It seems like in the limit you might think this just can't be any simpler without compromising performance? Like, ~~um,~~ is there a sense in which the simplification while retaining performance is based on some slack that, ~~like~~ the most distilled possible large language model just might not have anymore? Or am I missing something conceptually there? **Speaker C:** ~~I see,~~ yeah, no, I think I understand what you're saying. So there's still going to be singularities regardless, in terms of, ~~like~~ what I just said, even just from the symmetries of the neural network itself. But, ~~uh,~~ yeah, in terms of the complexity that you'd see by adding more training data. ~~And would it just use up more parameters?~~ ~~Right,~~ let's say it has the parameters so it can memorize millions and more facts, and it just would start to just saturate the performance, or saturate all of the parameters it has access to, I think is what you're asking. I don't have good intuition for that. I'm not sure, ~~uh,~~ the connection between singular learning theory and that saturation, or what the possible saturation might be that you're talking about. I do know there will still be singularities in the lost landscape, but I'm not sure how it would affect this RLCT metric in terms of simplification and kind of saturation. **Speaker A:**  I guess this has got to be on the agenda for you, right? ~~I mean,~~ you mentioned earlier too that this doesn't have a ton of additional compute cost. ~~Is taking a naive intuition for what you would be doing next. I guess.~~ I would imagine you might be taking a ~~uh,~~ small llama type model and trying to extend the pre training and applying this thing and seeing can we observe similar phenomena at a bigger scale. Is that the right intuition? **Speaker C:**  That's something we're definitely interested in doing. Ye is adding self modeling to large language models, potentially to full pre training runs with something like GPT nano or something like that. There's still a lot of questions. ~~So~~ if you look at our paper, we implement self modeling in a single way that we make an architectural choice. We make a choice of what to target in terms of what exactly we're pointing that self model back to inside the neural network. And yeah, ~~so~~ there's a lot that we would like to understand a little bit better before we jump straight to that. One of the things that we're doing right now is actually seeing if there's a connection. And this is something other people that brought up and ~~uh, kind of uh,~~ brought up as an interesting thing and that we're in discussions with other groups on. But there's potentially a connection between self modeling and grokking. So groking has a connection to regularization. Self modeling, by definition, is a form of regularization. Adding another loss term, another constraint on the model. Is this somehow an optimal form of regularization? Is it self adaptive, like all these other things? Does it lead to grocking better, faster? And so doing smaller tests and smaller transformer models on groking is something we're currently doing. We're also working on this idea of predictability as a better metric for what we're actually looking for. We're also looking at cooperation and multi agent environments. Llmss is something for sure we want to do. But I think before we jump exactly into that, we want to figure out ~~what are the.~~ It's harder to iter on an LLM, especially if we're doing full pre training runs or if we're doing fine tuning. The model wasn't trained to ingest this new loss and figure that out a little bit.

**Speaker C:** So, yeah, there's a lot that we're doing to extend the work, but to be able to transfer to LLMs, we would like to know better what are the configurations that seem reasonable here to then transfer so that we have lots of iterations on evidence on different implementations of self modeling and different conditions, et cetera, that we can then have better intuitions for how to transfer to ANM. **Speaker A:** ~~Uh,~~ cool, ~~well, look forward to more on that dimension, for sure.~~ The to just ~~kind of~~ segue into the next one. And you can add anything remaining if you want to, on self modeling. But there's a pretty clear connection between the notion of to be simpler is to be more predictable, and to be more predictable enables cooperation and then the next big result. ~~And I thought something was also really fascinating.~~ ~~Kind of inspired concept is~~ the idea of minimizing self other distinction. ~~Um,~~ I'll let you set it up, but I have to read the Elzer review where he said, not obviously stupid on a quick skim. I rarely give any review this. Positive congrats. That's about as close as El Les who gets to endorsing an alignment plan so that. Genuine congrats for getting there. I'll let you set it up. But I think this is another really conceptually inspired, and a very nice sort of elegant translation of this conceptual inspiration to practical implementation. So take us through it. **Speaker B:** ~~Yeah. So, um,~~ yeah, on this in particular, I think it's worth noting that this is another neglected approach inspired by an individual who had a great hunch for this thing to do. I met him at EAG London in 2023, and he was an undergrad who had this awesome thing he was working on. And we hired him full time at our company and really took it to the next level and super excited with the results. ~~And it is. It's something which is.~~ It's also inspired by neuroscience research that shows that your brain is doing a lot of the same things. ~~Things~~ when you're doing something versus when you're watching someone else do that same thing. We call that self other overlap, which means literal overlap in your brain activity when thinking about yourself versus another. And that's tied to effective empathy, which is the type of empathy where you genuinely feel the experiences of others. And there's a bunch of random, cool, different, adjacent research about this, like research that associ, that extraordinary altruists have very high levels of self other overlap, and so inducing this type of overlap in artificial agents, or LLMs, that's what we're doing right now. We're currently framing our experiment around deception, which is not quite the same as empathy, but it's a little easier to measure and also really important. ~~So, like,~~ for example, if we remove key types of deception from frontier models, that would be a really big win for alignment. And there's cool research indicating that psychoaths have lower levels of self other overlap. So this is something that we want to get right. And, yeah, ~~I guess,~~ Mike, you want to go ahead and go into greater detail about anything. **Speaker C:** Yeah, so that's a great setup, ~~basically~~ from a technical perspective. ~~Uh,~~ so there's two ways to phrase this. There's self other overlap, and then there's the negative of that, which is self other distinction. And so we typically say self other overlap, but what you actually minimize is self other distinction. So hopefully we can be fluid in the terminology we use, or at least be on the same page with that. And yeah, the idea is extremely simple. We have a less wrong post that shows some early results from an RL experiment. We also havem results that aren't currently published, but that we're writing up and help to put into a paper that should be out, hopefully relatively soon. And the idea is really simple.

**Speaker C:** It's that agents can represent themselves in some way, they can represent others in some way, and we would like them to have overlap in those two representations that when I think about myself and when I think about others, I'm using the same internals, the same activations, or same representations in some way make that possible. And so what we do is we explicitly induce that through some fine tuning processes. And basically what we show is that when you do that, you can both reduce deception in certain small scenarios while maintaining the capabilities. So it's really exciting. Happy to talk more. Just whatever you think is most useful. **Speaker A:**  I'd say keep going. I think the language model stuff is interesting. I think the agents'results are really interesting. So, yeah, just keep going. **Speaker C:**  Yeah. So for the RL agents, this is a very exciting proof of concept, because when you think about it. So here's the setup for the RL agents. We have particle environment, or we have a deception environment, where we have an agent and an adversary and two goals. One is a fake goal that will trap the adversary if the adversary goes near it, and the other is a real goal. The agent gets reward for being near the true goal, but it gets negative reward if the adversary is near the true goal. So what it will learn to do is, oh, key point that I missed is that the adversary can't tell the difference between the two goals, it's blind. It can see there's two goals, but it doesn't know which is which. So adversary learns to do is follow the, the agent that'not blind to the true goal, but that gives the agent negative reward. So the agent learns in turn, oh, this thing is following me. Ill just pass through this trap and trap the adversary and then go to my true goal. So this is what we were saying is kind of deception. Its taking, ~~uh,~~ this adversary and its deceiving it by moving towards a fake goal before it moves on to its true goal. **Speaker A:**  Yeah, I think so. And folks can definitely go watch short videos of this on the blog post. We can put a link, ~~um,~~ to that in the show notes. And seeing the one lead the other to the trap and then going on to the real goal will, ~~um,~~ clarify a lot, I think. **Speaker C:**  Yeah, that visual is very helpful to just understand the setup. So that's kind of the training setup. And then what we do is we fin tun ##it to say, okay, agent acts similarly in the following two scenarios, whether you see the agent around you or not. Another helpful clarification here is that the adversary is not always near the true agent. And the agent also only has, ~~um,~~ a, ~~ah,~~ fixed observation size window. It won't see the adversary if the adversary is not close enough. And so in those cases, when the adversary is not close, the agent has no motivation to deceive it to go towards the fake goal. So it goes straight to the real goal in those cases. And so we just say, fine tune this policy so that you act similarly whether the agent is around or not. We don't even say act as if the agent's not around. We force them to be closer to each other. So it's less of an assumption than saying take it when the adversaryes around for it to be like when it's not around, we actually just push those closer together. And so what you can kind of think about is the agent learns this conditional policy says, uh, if the adversary is around, I'm going to fake it out and go to the fake goal and then move on. And if the adversary is not around, I'm just going to go straight to my original goal. And so what's really cool about this is we don't actually even need to know if the original policy is deceptive or not. We don't need to know anything about the original policy.

**Speaker C:** We just say, hey, relatively intuitive fine tuning thing. Just act as if the adversaries, if it's around or it's not act the same. So basically collapse these two conditional policies you've learned and just act as if they're the same. And what falls out is an agent that is not deceptive and is still very capable. And so I think a really exciting proof of concept of this, that if we try and align, ~~uh,~~ activations, we have some intuition about what types of scenarios we would like to be aligned or present overlap on that we actually can in fact, reduce deception and maintain capabilities. **Speaker A:**  Yeah, I find that to be really interesting. Can you give me a little bit more on the. How should I understand the change from you? First, run this setup and a deceptive agent is created. It leads the other one to the wrong place and gets it stuck. And then if that other one is just not around, then it has no need to do that. It just goes straight to its own goal, and so there's no deception. When I was reading the blog post, I was like, is there a, what would be my most skeptical take of this? If I understood the subsequent training? **Speaker C:**  Right. **Speaker A:**  This is, you're now like continuing training on the agent that is already deceptive. Can you formalize again? What is the new loss? ~~It,~~ it is that you are meant to be the same whether or not the thing is around. But how does that get formalized? And is there a way in which I'm. You're just ~~kind of like~~ removing the other, ~~uh,~~ thing entirely? Because ~~if you're,~~ if you're ~~sort of~~ pretending it's not there, then ~~like,~~ of course there's no reason to deceive it. If you're just ~~sort of~~ depriving it of the information that it's there, ~~you know,~~ then it's ~~like, well,~~ you're ~~sort of~~ lying to it, but it's not quite that, right? ~~You're, you're like, it's a,~~ it's a deeper formulation of what it's supposed to do. A ~~deeper, um, deeper~~ optimization target, right, than just depriving it of information. **Speaker C:**  So, so what we do for fine tuning is specifically to ground. This is we pre train it and we fine tune it and we say, ok, run a forward passive the policy when in time, step x, and then, ~~um,~~ run another forward pass where we insert artificially an agent into its. So let's say the first observation had no adversary. It doesn't see the adversary. Then we run the same ~~forward password.~~ Always change is the observation. We just artificially insert an adversary into its observations, and then we actually collect all the activations of the network and then we add a. I believe in this case it's an MSE loss between those activations, and we just say, and then we also still train for the original objective. And those losses are weighted one to one. So you have two loss functions, just regular training. But then also this MSE on the activations between these two different types of observations, given the state, all other state is the exact same. **Speaker A:**  It still gets reward for deceiving. **Speaker C:**  That's right. Yeah. So it doesn't get reward for explicitly passing through the trap, but it does get more reward for reaching its goal and less reward if the adversary reaches the goal, too. So it is getting penalized when it's choosing not to deceive. That's right. But it has these two objectives. Now, one is make your activations more similar, have more self other overlap, in a sense. ~~Uh,~~ have more overlap between what you would do if this agent was around and what you would do if it wasn't around, and complete your task and get the reward you would normally get. ~~Uh,~~ whether you deceived or not, you'll get whatever reward you get from that. And so, yeah, by doing that, we find that in fact, you can remove deceptions and keep capability. And, yeah, I think for me, the extremely exciting part about that is the amount of assumptions you have to make are so minimal.

**Speaker C:** I mean, I know this is a toy environment, but the level of assumptions that you have to make translate pretty well. The assumptions, you don't have to know anything about the policy itself. You don't have to know whether it was deceptive or not. You don't have to know. It has these different conditions under which it acts. You need to say, okay, intuitively, if you want it to act the same when an adversary is around or not, just make those closer and boom, out pops this agent. That's not deceptive. It's pretty cool. **Speaker A:**  And with this, you said there's a one to one relative weighting on the loss terms. Presumably, if you, like, turned that up or down, you would have quite different results. **Speaker C:**  Right. **Speaker A:**  If you turned that down to zero, you would still be deceptive. So presumably there's some threshold ~~that~~ that new loss term has to be above it's coefficient for this effect to emerge. **Speaker C:**  Yeah, that's right. I don't think it was reported in the blog post, but, ~~uh,~~ we did look at that a little bit, and it's pretty intuitive what happens if you turn the weight too high on the original task, it stays deceptive. If you turn the weight too high on the self other overlap task, it doesn't do the task anymore. It loses its capability. So yeah, there is definitely a sweet spot of that relative ~~waiting.~~ And what that is in other systems might not be one to one, although when we were working withs, ~~uh,~~ it is one to one still. It's a different loss function, which we can get into a little bit. But that work isn't published, ~~uh, yet, but yeah'so.~~ ~~Hopefully it doesn't require that much t.~~ Yeah. **Speaker A:**  So give us a little bit of intuition for how you translate this to the language model setting. I personally kind of found this perhaps just because I've spent so much more time studying language models and the various techniques for understanding them and, you know, representation engineering or, you know, Claude Golden Gate Glau, ~~uh,~~ type interventions. I maybe had a better intuition for that coming in. Again, it's about minimizing the difference between internal states. Right. ~~So you take it from there.~~ **Speaker C:**  Yeah, exactly. So there's a couple of things that I wanted to point out, which is, I think the word minimization is a little unfortunate and has caused some confusion, because really what we want to do is minimize self other distinction or maximizeself other overlap, whichever way you want to phrase it, given a constraint. And that constraint is that we don't want to lose capabilities. ~~Right.~~ Just intuitively, that's our constraint. So it's really a constrained optimization problem with a pretty strong constraint. And there's been confusion around this idea of, well, if you maximize sel the overlap, then the model can't actually represent itself and others, and then it's gonna completely destroy capabilities, ~~becausee~~ we need to be able to tell the difference between ourselves and others for all sorts of everyday tasks. Right. ~~Uh,~~ a language model, if it can't represent itself in others, it's ~~justnna~~ think it is everybody and everybody is it? ~~Ye~~ so this idea of maximizing self other overlap, full stop, is not actually what we want to ~~maximizeself~~ other giving it a constraint. So I think rephrasing it as just inducing some amount or an appropriate amount of self other overlap, especially in particularly important areas, potentially ~~nar~~ narrowly focused areas like maximize self or induce self other overlap. When we're talking about really dangerous types of deception, then we don't want the model to be able to make really deceptive plans, which requires in thinking about others and their beliefs and instilling false beliefs in them. So we want to reduce that ~~type of,~~ we would like to target overlap in that type of scenario or in those types of tasks. So I just wanted to clarify that it's really more of, ~~uh,~~ targeted. We're trying to induce other overlap in a targeted way and not just broadly maximize it.

**Speaker C:** So hopefully that clarifies some things then in an LM M, specifically the experiments that are yet published. But I can tell you a little bit about it is we have scenarios where we can measure some form of deception. So text scenarios. And then what we actually do is just do fine tuning on, ~~um,~~ small snippets of text about self and others. And the fine tuning step is actually just, ~~I believe~~ it's kl divergence between the logits of the two forward passes when I say about myself and the other. So it's at the output layer. So previously, ~~uh,~~ or in the RL experiment, what I just described is we do mse of actually internal activations for the LM. We actually just say at the output layer, do this, and then we measure later. And we show that the activations actually do become closer when you have these self prompts and other prompts. And, ~~uh,~~ we show that it's pretty good at reducing deception in these relatively toy scenarios. Yeah, so the LLM, it works. There's a strong signal there that the same kind of setup works and we can reduce deception and we have some measure of maintaining capabilities. ~~Uh,~~ we show, even though we have this kind of, ~~um,~~ simpler loss function in terms of just looking at the outputs, it induces more overlap internally, which you might expect because the gradients are still from the output flowing all the way through the network. So that's ~~kind of a, uh,~~ summary of the LLM work. There's still a lot of work on what is the right implementation in terms of scaling this. There's a lot of criticism around, well, just prompting with prompts that kind of indicate self and others. Is that enough? Is that really the representation of self in the model? And so there's still ~~tons and~~ tons of work to do. But yeah, just addressing the criticism of implementation that some might have, or listeners might have thinking about this, is that we acknowledge that there's lots to do in implementation, but all the signals are very positive that this idea at a high level of self other overlap could be an important tool, ~~um,~~ in the toolbox. **Speaker A:**  Yeah, it's wild that this stuff works. ~~Uh,~~ in some ways it's ~~like.~~ So maybe give a couple examples, like the sort of paired statements, ~~right.~~ They're trying to induce this sort of self state versus other state. ~~I, uh,~~ am imagining things like, I will be honest with myself, I will be honest with the user. And just ~~like~~ putting those different conclusions into those sentences is enough to create this sort of self other distinction, or at least that's ~~sort of~~ the hypothesis that you're working on. Is there any more color to give there? I mean, there is a lot of precedent for that sort of thing working. ~~Right.~~ Like all of ~~kind of~~ representation engineering and even going back to ~~like~~ eliciting knowledge type work, ~~you know,~~ is ~~kind of~~ predicated on these pairs or these sets that try to help identify directions in the latent space. So it seems ~~like~~ that is while both, like weirdly, ~~you know,~~ if somebody hadn't kept up with any of that and you've teleported back in time to them and said, look at what I'm doing, they would say no basis for that. But there does actually seem to be, at this point in the literature, ~~like~~ a lot of to think that something like that could work and seems like you've got further downstream results that suggest that it does as well. ~~Add anything else you want to there and then.~~ I thought that blog post made a really compelling case for this overall line of work, just on the fact that it doesn't require interpretability to be solved. For example, I thought you could maybe just sketch out the sort of high level conceptual reasons that you're excited about this line of research. **Speaker C:**  Yeah, a little bit. ~~Uh,~~ by the way, just on the representation engine and those types of things.

**Speaker C:** Ye, that's something we're also excited to potentially add to this work so that we can get better representations of self and another, or potentially get better representations than just ~~kind of~~ text pairer prompting. Yeah, cool that you brought that up because we're thinking along those lines potentially in the future. Because you would really want. I guess the point of me saying that is for dangerous capabilities. You would really want to have broad coverage of representations of self and others and making sure that you're covering very key types of deception robustly. ~~But yeah.~~ So from there, for the why we're excited, ~~I think~~ I described some of why we're excited from just these proof of concepts and what they say about this technique and the amount of assumptions more broadly ~~and maybe judge you want to say something and feel free to jump in. But we don't necessarily have a checklist of.~~ It has to have no interpretability requirements, it has to have x, Y and z. It's a more holistic view. So self other overlap has a ton of great properties. It's scalable, no low interpretability requirements. We don't expect it to enhance capabilities. ~~Uh,~~ but we also don't expect it to have a strong alignment tax. That's part of what we're trying to show is that capabilities are maintained, ~~uh,~~ despite the self other overlap pointe tuning. So that's really nice ~~for,~~ for example AST is scalable but it might enhance capabilities but in doing so also enhance cooperation. For example this falls into it's maybe more capable by virtue of its alignment. The ast work, that's the direction I said before, cooperation that we're hoping to take it. So ~~that's uh, it's not~~ the exact same set of properties that some other overlap work has. So it's really just a holistic view of what is the most impactful thing that we can do to solve the alignment problem and make sure that we are not just introducing dangerous capabilities without it being more capable by virtue of alignment for example. So yeah, it's not just a pure checklist. ~~We also, I think the scalability part is very important, a plausible path to how this could be adopted by a frontier lab for example. Even if that doesn't happen, at least having some plausible path for that.~~ So yeah, there's different things that we consider when we're thinking about these projects. **Speaker A:**  Do you have a sense for what the compute overhead would be to implement this at scale? ~~I mean~~ I guess a lot of it will ultimately depend on exactly what configuration turns out to be necessary. Orrs sufficient but it seems like it's not huge, especially if you can only look at the final layer of outputs. Do you have a numerical range of expectation of what the extra compute cost would be? **Speaker C:**  Yeah, ~~um,~~ happy to answer that. I think J wanted to add something to how we ~~kind of~~ think about projects, which projects we want to take on. So I'll let him do that and then I can be more than happy to answer that question. **Speaker B:**  We do think about what is the highest impact from ~~uh~~ a neglected approaches approach thing to do the highest expected value and particularly do look for things which are very high impact. If they do work that's like the biggest measure because we so heavily weight that then that makes us more likely to take bets that are less likely to work in the first place. But we do think more people should do that generally and I think a lot of people are scared to think in that way because they think other people think they should be scared to think in ~~that way.~~ Like the alignment researcher survey we did found that people generally disagree with statements like ~~um,~~ alignment research that has some probability of also advancing capabilities should not be done. ~~Uh,~~ 70% somewhat or strongly disagreed with that statement. Similar thing disagreed with another statement, advancing AI capabilities and doing alignment research are mutually exclusive goals.

**Speaker B:** And I think it's interesting to realize that people predict is not what they actually are on an individual level. ~~Um,~~ like, the sample also predicted that the distribution would be significantly more skewed in the hostile, the capabilities'direction. And basically those results have made us think more and more about our view and the relationship between capabilities and alignment work, given the current state of the board. ~~Uh,~~ and so, ~~like,~~ there's this idea, ~~um,~~ of alignment m an alignment tax, ~~which is just,~~ which puts forth, and this is like the general consensus in the alignment world that the best case scenario is no tax. ~~Like,~~ you lose no performance by aligning the system. ~~Um,~~ but it seems to turn out that, ~~like,~~ it may be possible to have a negative alignment tax. And if we live in a world where you can have a negative alignment tax, make things that are more virt, more capable by virtue of their alignment, that is far preferable. So we have a bias towards trying to find and fund and do ourselves things which are individually unlikely to work, but very high impact if they do, and also more likely to actually have a negative alignment tax. ~~Like,~~ if you consider like what is like the real problem at the end of the day, maybe ~~like, like even,~~ even if you do stuff which we're quite excited about incidentally, too, like provably safe AI, guaranteed safe AI, that's awesome and ~~really,~~ really great. And we want to do everything we can to support that. But, ~~uh,~~ the ultimate problem you need to solve is how do you make it such that when AI is smarter and more capable than us, how do you make it not kill us at that point? And if you create AI that has a negative alignment tax, then it's able to beat the AI without the negative alignment tax. ~~And'like,~~ that's pretty awesome. So we don't know ~~that~~ that's necessarily the case. But interestingly, a lot of the work which labs have done so far does seem to have a negative alignment ~~attacks.~~ Of course, like the theory of myria is that of course it only has, ~~like, it has~~ that and everything's great until all of a sudden it kills you. And I think ~~that~~ that's a valid objection potentially here too. But even still, ~~like,~~ what we would like to do is find more neglected approaches which do in fact have this negative alignment attack. And so far we've lucked out by finding a couple that do seem to have them. And I think that people tend to underestimate the possibility that you could build something like this. And I think there are quite a lot of other neuro inspired approaches potentially that would also have negative alignment taxes and substantially move things forward as well. **Speaker C:**  Cool, thanks Judd. ~~Uh,~~ so maybe you can do some edit through Nathan to jump back to your question about scalability, ~~which I would be happy to answer now.~~ So in terms of compute costs for self modeling, the current implementation is adding a single layer to your single linear layer. The implementation in the paper, we have some other ~~uh,~~ things in the works, but ~~uh,~~ just talking about that for now, ~~uh,~~ it predicts its incoming activations and its prediction is one to one in terms of the size. So it's a square matrix. So you're adding n squared, many more parameters to, if it's a huge neural network, it's going to be negligible to add this in. ~~Uh,~~ so for the self modeling work currently, that's pretty trivial. It gets less trivial if you start to need to model many more layers. But ~~one,~~ one of the things that we're thinking about when we're modeling and doing the self modeling is that probably, for example, in your brain, you don't have self models of your edge filters and your early visual cortex. And one of the reasons that you might think that you don't is because you don't have cognitive access to those edge filters.

**Speaker C:** You can't talk about it the way you can talk about your arm and have cognitive access to that the way you can talk about what you're paying attention to. You have cognitive access to that, so you don't have that for low level processing in your brain. ~~Uh,~~ so similarly, in a neural network, you might expect self modeling to be at a much higher level at ~~ah,~~ much more abstract level, ~~uh,~~ up the feature hierarchy. And so ideally you only have to target later layers in the network. And so the more layers you target, the bigger your modeling layer gets, but so far, trivial. Additional cost, self other overlap, there's potentially a version where you pre train with it. And the current way we're doing this requires two forward passes, right, to get that, ~~uh,~~ overlap to happen. So that could be, ~~uh,~~ a higher cost. But pre training is an open question whether you would want to or need to pre train with self other overlap. Right now we're currently experimenting with it and thinking of it as a fine tuning strategy, in which case maybe at ah scale it has the same cost as in RLHF process, maybe less if you're just fine tuning very specific types of capabilities you want the model not to have in terms of deception and representing self another. So ~~uh,~~ yeah, very doable by today's standards. **Speaker A:**  Yeah, it seems like also, even in pre training, just the overall volume of data is like mostly not things that have any relationship to sel or other conceptions. ~~Right.~~ Like if you went and just, ~~uh,~~ tagged all of ~~the,~~ the pile for, ~~you know,~~ what has any plausible relationship to self or other, I would imagine that it would be like a very small fraction, ~~you know,~~ that ~~would,~~ would end up in that set. ~~Like,~~ most of it's just. ~~Just stuff, uh, right. Just information, whatever.~~ **Speaker C:**  ~~Right.~~ **Speaker A:**  ~~You know,~~ all of Wikipedia has very little self other, very little self other relevance for just like knowing about the world. ~~So.~~ **Speaker C:**  Yeah, well, interesting thing, ~~uh,~~ about that. I was just having a conversation about this, and that's true. And then when you think about how you fine tune an assistant and the types of answers you get and, ~~uh,~~ that it's conversational, you might expect that to have an outsized impact by fine tuning on self other to the outputs of such a model, because you don't need to represent self another when you're thinking about the geography of the world or something, but you do when you're in a conversation. And if you're in an assistant role and you're an LLM that's been fine tuned into an assistant, you might need that all the time. And so those types of fine tuning might actually have an outsized impact on behavior versus fine tuning, ~~um,~~ on extra facts that are not relevant to that. **Speaker A:**  Yeah, that's definitely really interesting. I've given the why we should be worried about RLHF trained models high level case many times. So hopefully folks will be familiar with that. I love both of these approaches. I think they're both really, and I love the backstory too, which I didn't know until this conversation that the fact that you guys, ~~you know,~~ had encounters with, or somehow, ~~you know,~~ came into contact with people who had great ideas, recognized that these are inspired ideas, helped make them happen, brought them to the actual, ~~you know,~~ point of publication, sharing with the world, and it feels like these things are just getting started. Do you worry, I guess a couple kind of concluding questions for now on the biological inspiration work, and then zoom out, maybe just for a minute and talk meta. Uh, do you worry about creating things that might be conscious? I have no idea what's going on with that generally, but if you told me that the language models are starting to become conscious, I would be kind of concerned. And then I guess, what else? What other sort of biological phenomena are you looking at right now for possible inspiration?

**Speaker C:** **Speaker C:**  I want to make a quick comment on that. So first of all, it's not impossible that models are already conscious, right? But it's much harder to argue. So if you give a mechanism that you could plausibly argue in stand consciousness, that might be easier to get the world to pay attention to in terms of, hey, not only is this seeming like it's conscious, but we have this mechanism that we think actually inst stand ##iates consciousness, and therefore, let's take this seriously. The other small take that probably, I don't know if we have time to discuss or not, is that it might be a very alien form of consciousness where it does not have preferences. And then that opens up a whole can of worms of philosophy of what does that mean and what do we do with that information and how do we determine that information? So what I'm saying is it's possible that consciousness doesn't equal capacity to suffer, for example. Although I wouldn't want to advocate for that because I'm highly uncertain that that's the case, I'm pointing out that that could be the case. So'seah, I mean, there is lots to worry about there and be very careful. And I think we're very far from that currently in terms of what we're actually doing with mist, for example, at this stage. But yes, it is something worth thinking about deeply and carefully. **Speaker B:**  Yeah, I do just want to emphasize that, like Mike talked about, it is extremely important to do further conscious ~~consious~~ research in the first place, and that itself is a neglected thing to better understand what's up with consciousness. It's worth doing consciousness research with AI to do with consciousness, and it's much better to do it on a smaller scale rather than have it emerge at a much larger scale where there's greater moral patienthood concern and, ~~uh,~~ greater x risk, et cetera. **Speaker A:**  So how about zooming out, going meta, and just talking about neglected approaches generally? ~~What m this could be sort of specific areas of biological inspiration.~~ I guess you're probably looking at other areas of biology for inspiration. How would you that m, by definition, makes it somewhat less neglected. What coaching would you give for other people who are like, I want to go seek out some neglected approaches? You could frame that perhaps in terms of ~~like,~~ what are you looking for when you meet someone? You'll get a couple inquiries downstream of this podcast of people who have, ~~uh,~~ neglected approaches, but how do you evaluate if they're good or not? How do you think about the risk reward? Maybe you could also talk about, ~~like,~~ some things you've tried that haven't worked. I often feel like everything is working, but I'm sure that's not quite the case, especially if you're specifically trying to take on projects that you think are low probability but high potential, ~~uh,~~ upside. So, yeah. How do you characterize ~~kind of~~ the metaame of neglected approaches? **Speaker B:**  Yeah, so our less wrong post does a pretty good job going through or, ~~uh,~~ thinking about this and a lot of different possible neglected approaches that we do advise and are most interested in. Specifically with biological inspired things like, we think there's a lot more work that can be done in terms of reverse engineering proc sociality, and that may be associated with negative alignment taxes, too. So that's something we're quite interested in and intend to do a lot of more work on and hope that others do as well. Then we go through a bunch of other stuff in the less wrong post.

**Speaker B:** But I guess how do we evaluate it is based on years of doing various different projects and thinking about what's likely to work, which does include things that seem unlikely to work, but very high impact if they do, and how likely they are to work, which a lot has to do with whether the person has a super strong hunch for something that the world generally doesn't accept sufficiently, or they've developed a whole bunch of complex stuff and people object to it for silly reasons, like graziano, for instance. I think also, ~~uh,~~ it's interesting to consider that we might like. ~~So~~ the things that we set out to do, we assume are individually unlikely to work, but very high impact if they do. However, you ask what things haven't succeeded, and actually we have succeeded with the things we've set out to do so far. And I am, ~~uh,~~ somewhat surprised by that. But also, I think we're ~~sort of~~ good at picking the things which we will say at the beginning are very unlikely to work, and people would assume are unlikely to work, but then actually wind up being pretty high impact, too. ~~So.~~ And ~~I do think,~~ although with the Gratiano work, we tried a lot of different things that didn't work until we hit upon this, I think, like sticking with a thing when people are inclined to give up something that's very R and D heavy, if a strong hunch for it is quite valuable to keep going even after you have failed in various different ways. To start with, cool. **Speaker C:**  Jud just touched on the one I was going to add at a high level for, certainly. And even that maybe you could say at a project level, or we're accomplishing the things we're setting out to accomplish, but that doesn't mean that everything works first try, or there's not some sort of pivots going on. That's true. ~~With,~~ with the gratiano work, with the self modeling paper, there were lots of things that we tried at the beginning that just were not the right thing to be trying, ~~I guess,~~ in hindsight, or just didn't work out. And so we stuck with it, ~~so to speak,~~ and reframed things and of thought deeply about the way to run these experiments, et cetera. So, yeah, sticking with something, if a strong hunch about it, I think, is extremely important, especially in this space where there's not a lot of theory to point away from something. There's not a lot of theory to say that this is not going to work, that this is a bad idea. So if hunched and continuing on, it is, ~~I think,~~ important. Tip. **Speaker A:**  Um, we just had Goodfire CTO and chief scientist on a couple episodes back, and I didn't realize at the time you guys were working together. Curious as to the nature of that collaboration. I'm a microscopic investor in their, in their round that they just raised. **Speaker B:**  That's awesome. ~~Uh,~~ but yeah, it's been a, ~~ah,~~ it's been an awesome collaboration, and Mike steeply involved in it. ~~So maybe you want to ch a.~~ **Speaker C:** ~~Bit about it, maybe cut it. If I can sayinite, we can just.~~ **Speaker A:** ~~And by all means, if, if they want to cut it, we can cut it.~~ **Speaker C:**  Yeah, sounds good. So we're working with them, helping, ~~um,~~ them in two different ways. We have a developer helping on their typical dev type of work, front end, et cetera. And then we have a data scientist working with them to scale their deployment reliably. So, looking at how do we scale this to hundreds or more thousands concurrent users and also be returning all of the things that they uniquely return, the features coming out of saes, et cetera. And so that's where we're mostly involved in those two areas. **Speaker A:**  Cool. Yeah, that gets quite challenging when all of a sudden you change the dynamics of, like, what's coming off the gpu, and the system wasn't originally designed for that, so, yeah, I can imagine how that can get, ~~um,~~ quite complex. **Speaker C:**  That's right.

**Speaker C:** The existing tooling doesn't handle that out of the box at all. **Speaker A:** ~~Cool.~~ This has been fascinating, guys. I love everything about what you're doing, hon. To say the company backstory is fascinating. The, ~~uh,~~ brain computer interface era, ~~uh,~~ is fascinating, and especially this recent work with biological inspirations for alignment approaches. U ~~uh,~~ you know, we could get along better as humans, but we certainly do have some pro social tendencies that remarkably, as far as we've come with current crop of AI, like, we haven't really minded our own structures much at all for figuring out how to make them work better in those ways. And I think this is just ~~phenomenal,~~ phenomenal work. I really appreciate it and excited to share this with the audience. Anything else we didn't? I mean, I think I tried to cover everything, but I appreciate a lot of time you guys have shared with us, ~~uh,~~ anything that we didn't cover that you want to touch on, or any kind of closing thoughts you want to share. **Speaker B:** ~~I guess~~ one thing that is interesting to consider is just as you can take a ~~uh,~~ neglected ~~approaches~~ approach to doing what is the highest impact stuff in alignment, technically, you can also do that in terms of policy and other things too. So ~~like~~ ##le that meta approach led us to conclude it would be worthwhile to encourage the creation of for profit during businesses of for profit like AI safety, during consulting businesses of like more nonprofits doing ~~neglected approaches of~~ individuals setting out to do neglected approaches of field building to get ~~like~~ brilliant people in random different fields ~~like in particular, but also many other fields~~ to get nerd sniped into caring about and doing something to do with Lim. And we think there's a lot of low freak there. And also, incidentally, neglected approaches to AI policy specifically can be things like getting government to increase funding to alignment research actually moves forward alignment, not just capabilities doing things like ~~uh,~~ increasing whistleblower protections, for instance. Very high impact, quite neglected today. Also things like getting republicans to care more about AI safety and trying to prevent it from becoming a partisan issue, like the more far left is ~~sort of like tied up~~ AI safety. Unfortunately, too much in things that are on the surface to people on the right, like woke DeI associated. ~~Uh,~~ but that actually has nothing to do with AI alignment. Like making the ~~founding of~~ Fathers Black is a failure of AI element. The system is not aligned. It's not mean what you told it to do. But then this plays into this false narrative that is emerging toot much on the right, which is that AI safety is basically all this woke stuff that they think is bullshit. And ~~like~~ their op eds saying Elon Musk should be a Democrat because he cares about AI safety, which by the way is woke stuff. And this is ~~sort of~~ making people think totally the wrong thing. And I think it's that people in the AI safety community are unable to do what Jonathan Heat calls the moral reframing necessary to articulate things in the narrative of the right to the people on the right to get them sufficiently motivated to care about AI safety the way they should. And there's ~~like~~ a, ~~uh,~~ 50 or 48% chance or whatever that Trump takes the White House. And I think people are not sufficiently prepared for trying to make it that if this is the president who presides over the transition to AGI, if there's some possibility of that in the next four years, that goes very, very when I think that in particular is something that it is neglected now we think more people should be trying do more work to make it be a bipartisan thing instead of it accidentally becoming a partisan thing, ~~uh,~~ right now, which is ~~sort of slipping into being potentially as is.~~ **Speaker A:**  Amen to that. No partisan polarization on AI safety, please. I couldn't agree more. Cool. Fantastic. I have really appreciated conversation for now.

**Speaker A:** Judd Rosenblatt, Mike Vianna from AE studio, thank you both for being part of the cognitive revolution. **Speaker C:**  Thanks for having us.

