**Speaker A:**  Dertron, CEO of Metaculus. Welcome to the cognitive revolution. **Speaker B:**  Thank you. Big fun here. **Speaker A:**  Thank you. That's kind of you to say. I'm excited for this conversation. I've been a metaculous watcher and am I saying that right? ~~By the way, let me make sure I'm pronouncing the company right, too.~~ **Speaker B:**  Yes. Yes. **Speaker A:**  ~~Ok.~~ Metaculous. **Speaker B:**  We got a lot of meticulus and meta calculus, ~~um,~~ which are both reasonable. I like what those signal as well. But metaculous is what we go. **Speaker A:**  All right, cool. So I've been a long time watcher, and you've got some very interesting new projects, which will be kind of the bulk of our conversation today.



**Speaker B:** But the fact that it is copy paste does not make it illegitimate. ~~You know,~~ I will read, ~~you know,~~ a campaign and say, yeah, I agree with this, I want to send this. How should you take that into account compared to, ~~you know,~~ something that is bot generated compared to something that is you? A genuine opinion that someone has taken time, but it's not necessarily sophisticated and you can see that it has shortcomings. And, ~~um,~~ in my thesis work, I was looking at this both from a legal perspective and also technical perspective. ~~You know,~~ what are the expectations from, ~~you know,~~ EPA and EPA perspective on just the us legislation, but also, how can we have natural languageing augment this? ~~Um,~~ and funny enough, at that point I was thinking a lot of questions of good hearting. ~~Like,~~ every time we start focusing on a specific, narrower metric, it ceases to be a good measure. And this was very apparent in being able to claim that you're looking at the public. ~~Um,~~ things we tried were so simple, and yet they seemed ~~like, ah, oh,~~ this is a paradigm shift. ~~Um,~~ just like simple clustering strategies, unsupervised learning topic modeling. ~~Um,~~ and even that went a long way because people had no idea, and now we can do much more sophisticated versions and people still are like, people assume that we don't need to ask these questions because we cannot do large scale qualitativeveys. So we keep going back to Likert scale. So I guess the crusade I've been on for a long while is, can we go past, ~~uh,~~ how happy are you, one to seven, or should we build a road from a to b or a to c. ~~I'm like,~~ maybe we need to build a bridge instead. And if you don't let people express that, you're just not going to get that opinion. So how do we get, ~~um,~~ machine learner systems optimizers to be able to understand? Uh, there is something qualitative that I need to elicit that even the speaker might not be sure what they want. ~~Like,~~ how can we give room for that human flexibility in these misschions? ~~I cannot hear. Sorry.~~ **Speaker A:** ~~There's a little construction outside my, uh, window, so I'm muting the mic sometimes, but.~~ ~~And then I've got to remember to unmute it.~~ ~~Um,~~ so your comment on goodharting definitely resonates with me, particularly today. I was just, ~~uh,~~ having some back and forth on Twitter about the new GPT four o mini model, which is coming in ahead of claud 3.5 sonnet on the, ~~uh,~~ LMC leaderboard. And that raises the question. It seems like the general consensus among people like me who are obsessed with the latest AIs'and, ~~kind of~~ have their own individual opinions, seems to be that 3.5 sonnet is the best. ~~Um, certainly we all would~~ all tend to think it's better than GB 400 mini, and yet there it is, ~~uh,~~ ahead in terms of the votes that it's getting. So this has raised all sorts of why is this happening? Do we trust this thing anymore? ~~Um,~~ is it just about formatting? Is formatting. If it is mostly about formatting, does that mean it's ~~like,~~ illegitimate in some way, or does that mean we should really think more about the importance of formatting? ~~Um,~~ but yeah, it's very. As always, it's extremely difficult to pin much down if you really want to do it, ~~um,~~ with any real rigor. So that, ~~um,~~ is maybe a good bridge toward. And certainly the same is even more true about the future. So I've been interested in this space of forecasting for a long time. I actually participated in one of the original, ~~um,~~ tetlock organized super forecasting tournaments maybe 15 years ago now. ~~Uh,~~ Tyler Cowen posted, ~~ah,~~ an invitation to participate, and I was like, I'll try it in my hand at that. I did reasonably well. I wouldn't say, ~~um,~~ I didn't top the leaderboard, but I came out feeling like I had at least somewhat of a knack for it. And I've always found that the sort of Robin Hanseson perspective on this seems like a much better way than ~~sor~~ of highest paid person's opinion or whatever kind of prevails in organizations. ~~Um,~~ that's always been compelling to me, but it seems like generally speaking, we still don't see a ton of forecasting deployments in the world, I guess. What would your analysis be of ~~kind of~~ where we are in forecasting, where you think the most notable deployments are, and why we don't see more than we do today? **Speaker B:**  Right.

**Speaker B:** I want to answer from the previous point you made around good hearting, because I think this actually goes towards there and you know, which model is better? Define better. Right. Better is incredibly content specific. Like even in just a small world of things I am trying to do personally within metaculous clau like 3.5 is more useful for me on question writing, but not necessarily fact retrieval. And it's a very narrow thing. And this current version of these models seem like, ~~you know,~~ one of them has an easier knack towards one thing without interrupting, without self censoring, so I can move forward faster. I think the core of forecasting also relates to this in terms of how can this process actually be useful? Is one thing that is different from, ~~you know,~~ will it score good on these benchmarks? ~~Like~~ in this one specific competition, will, ~~you know,~~ 4.0 score better? ~~Like,~~ that is a much narrower thing. In a way. Usefulness is almost always anecdotal and I think we are not paying enough attention to that. And I think the same as applicable for forecs. Well, there has been so many tournaments like latest, know, like ACX ~~uh,~~ 2023 tournament is interesting for how accurate the forecasts have gotten. ~~Um,~~ but to me, okay, how is this going to actually be useful for an end user is a different question. And I think this is the approach that I really want to focus on with metaculous for the upcoming sprint is, okay, we, I have a couple of validations that, ~~you know,~~ I feel satisfied with. What are those? One wisdom of the crowd works. It is interesting that it works, but when you aggregate a number of people, and we will go into it, when you aggregate a number of bots, ~~um,~~ that are trained with LLMs or what have you, it seems to be able to draw an accurate picture of the future. And we have enough case studies of this. Why is this not more commonly used? I think usefulness is a whole different paradigm than accuracy, and this is really what I want to focus on. What do we want Toa do on this front? Like for example, can we identify critical inflection points towards future states? Is a different question than just is your forecast accurate? ~~What is the shape of the world?~~ That we want to live in. What are entities? ~~You know,~~ these can be federal agencies, like my prior work. This can be philanthropists, this can be corporations, nation states. The framework of forecasting so far, especially from a te talking lens, I think has not focused on how will this be actually useful, but has more so focused on can we improve the accuracy. I would like to move forward towards coming up with versions of forecasting deployments that actively pay attention to how will the decision maker take this into account? What are things that are within their space of levers? This is why I think forecasting in context of the humans that are trying to live a better life is extremely important. ~~Um,~~ I can share more examples on this front, but I'm curious where you'to go. But, ~~um,~~ with this, like pretty much our research agenda for metaculous, for the next chapter, for the next, ~~um,~~ two quarters heavily focuses on a lot of experimentation on these kinds of questions. **Speaker A:**  Yeah, interesting. Does that look like basically conditional. **Speaker B:**  Markets. **Speaker A:**  ~~Um,~~ being kind of the first thing, I mean, that's something that Robin Henson has talked about for a long time, and I don't see much, but we've just lived through a moment in history where it seems like something like that was pretty motivating to the current president of the United States, who I, ~~uh,~~ think was, if reporting and general intuition are correct, seems like he was not sold on the idea that somebody else had a better chance than him until he saw polling data suggesting that and then was kind of like, well, I guess, yeah, if somebody else has that much of a better chance, maybe I should step aside. ~~Um,~~ is that the sort of. Obviously play that out a million different ways for a million different context. But is that the kind of next big paradigm? **Speaker B:**  I think that is an example that basically is, uh, a very simplified and somewhat was obvious to the entire population, except the decision makers themselves for longer than it should have, in my opinion, state of affairs. With that said, the real cases that I'm interested in is much more sophisticated than this, right.

**Speaker B:** That heavily looks at public opinion and is more, I may say, like vibes based rather than data driven. And I'm interested in really pushing this edge further. Say we have $50 million to allocate towards making the world better. Pick your challenge. You reducing microplastics, climate related risks, AI related risk. How should we allocate these resources to get to a better world? Requires us to build world models. I think forecasts around what are specific outcomes we can get to if we take an intervention point is very helpful. And the simplest thing we can do is exactly what you said on you come up with conditional forecast. If we take lever x, will that bring us to a better future? If not, what will it be like? ~~Um,~~ and if we see high divergence here, that is great. Now I want to push this much further. We can already do this. ~~Um,~~ but ~~through~~ test conditional questions are somewhat hard to answer. It's hard to think in that framework. So can we build tools, be it, ~~you know,~~ LLM driven or discourse driven, or come up with ways in which people can collaborate, that enable conditional forecasts to be more useful? This is one avenue. One of the things I want to do in, ~~uh,~~ the near future is launch a series of ~~minitacul.~~ This is the name we came up with. Think of it like subreddits, which is metaculous instances. This will come ~~in~~ context of us open sourcing metaculus, and we hope that many of these will spring in the next quarter. ~~Um,~~ each Mintaculus will be focused on a specific question, a goal oriented question, such as we are trying to reduce microplastics, for example, or we're trying to reduce homelessness in San Francisco. This is geared towards disaster response or consequences of research avenues and ~~context of~~ AI. I would want every question in the minuteacul to be ~~able~~ to serve. If we are able to answer this question, it bubbles up to the parent, and then we can see which of these questions have the highest value of information. Which of these questions actually inform us to say, oh, it looks like this intervention is going to be able to drive much further value. I would want us to identify critical inflection points like, is there a specific moment in which the world models see to diverge? Are we able to extract schools of thought in the context of forecasting by seeing, oh, ~~uh,~~ this group of forecasters seem to be much more in accordance over multiple forecasts. Why is there world modern divergent? Can we double down on finding more questions that can help us excavate that? These kinds of research questions go way beyond what metaculous has aimed for so far. This is actively trying to build a world model in an enclosed world space that is metaculous with a spec goal. And I find this to be ~~really,~~ really interesting. ~~Like,~~ the kinds of things I would love to do is, for example, can we find short term proxies and heavily forecast on both? Is this short term proxy good? And how is the short term proxy panning out? ~~Um,~~ the way we landed with the fab, the, ~~uh,~~ forecasting AI benchmark is basically guided from this. Also in a way, ~~um,~~ we can frame the presidential debate through this lens. I think we as a civilization need to think much more from this lens. Like this is a version of a cognitive revolution where we change how people are thinking about the future. In a way, we're enabling them to coordinate between their world models. I still see forecast aggregation ~~to be~~ fairly low fidelity to be able to coordinate world models, but I think we can use that as a building block, as a primitive, and come up with much better world models. **Speaker A:**  So let's maybe take one step back just for those who maybe don't have so much experience with metaculous as it exists today. ~~Uh,~~ I'd be interested to hear ~~just like~~ for starters, how do you describe it to somebody who's never seen it before? ~~Um,~~ would you call it a prediction market? Would you call it a forecasting platform? ~~Um,~~ how does it fit it? Because there's like a few of these now, right, with people would be familiar with certainly just online betting platforms, but also there's like manifold and polymarket that I see ~~kind of, you know,~~ shared them around. How would you describe the overall product today, and how would you distinguish it from some of the other things that are out there? **Speaker B:**  Mhm. Metaculous aggregates forecasts. It's a forecasting platform and an aggregation engine.

**Speaker B:** I would not call metaculous a prediction market. The goal of Metaulus is to bring a level of epistemic security towards how we envision the future to be. We can call it Wikipedia about the future, and it is a space in which multiple people can see how they foresee the future to play out and the critical discussions to take place there. ~~Um,~~ it works with predictions as its core block, but it does not have a monetary incentive. ~~Um,~~ it behaves quite differently than prediction markets as a result of that. And some of those differences are the reasons why, in my opinion, Metaculus has been both more accurate and also more rigorous compared to a lot of other spaces. ~~Um, uh,~~ I can shed more light into that. For example, on a prediction market, the, ~~uh,~~ participants are buying and selling contracts that are based on the future outcome of an event, ~~right?~~ So you place bets and the result is a zero sum question. For example, if the current forecast is at 60% and you think it is 60%, you don't have an incentive to add that information in because it only can get worse from there. While on metaculous, this is not the case. ~~Um,~~ there's no financial incentive, and instead we are comparing all the forecasters accuracy and track record through time. And, ~~um,~~ this creates a different set of incentives that I believe are actually much more productive towards something that looks like Wikipedia about the future, where people are one incentivized share what their world model is. Two, people are incentivized to participate in forecasts and not think about de risking or spreading, but instead focus on, ~~you know,~~ what is the world model that I have that I can share. So our scoring mechanisms are different with respect to prediction markets as a result of that, too, because the reward mechanism is that you reward a forecaster over time for their calibration across many predictions and for their accuracy. ~~Um,~~ we also have a metric that we call the community prediction. This is interesting because this is where you can see everyone that has predicted on this question. And we also have weighted averages depending on your past success on your track record, and the recency for sooner forecasts thand to be better for, you have more information. So using these, we actually end up in an ecosystem that is much more collaborative and much more grounded and good epistemicics. And this brings higher rigor as well. So the questions we forecast tend to be more like, ~~you know,~~ carbon emissions on this d, rather than very personal questions, ~~uh,~~ on you. Who am I gonna be dating with next? And I like that, too. ~~You know,~~ there's a market for that, but I like that there is a space for focusing on the rigor that is actually beneficial for the world. So that is what I find distinct and attractive about metaculous. **Speaker A:**  Gotcha. So it seems like the big thing there is the incentives, whereas on a more market oriented platform, I am looking for things that are wrong so I can make money here. I am looking for just opportunities where I can share something with the community that hopefully brings the overall, ~~uh,~~ community forecast toward a more accurate. **Speaker B:**  ~~Right.~~ But on top of that, I will add, there is a financial ~~incentiment,~~ metaculous, too, as in, if you have very good track record as a forecaster, we will hire you as a pro forecaster to go engage on many specific paid client works. And these range from federal agencies to large retailers, to hedge funds, think tanks that have much narrower questions. These questions don't make their way to the public forecast that we have, and they will be paid engagements, and we will also pay the pro forecasters for their reasoning so that they can actually explain why they see the forecast to be this way. Just to echo back to a thing we were talking about, I think one of the core projects that we have for metaculous is going to be reasoning transparency. The forecast on, say, I'm making up an example, ~~um,~~ say we'thinking about the taiwanese sovereignty. This was the context in which this came up. ~~Um,~~ I was in Taiwan recently for a conference, and we were talking about, ~~um,~~ will Taiwan, ~~you know,~~ electricity grid be, ~~um,~~ challenged by China, as, ~~you know,~~ in the process of a potential invasion? And, ~~um,~~ the forecast jumped from 30% to 40%. And there were no comments under it because it was fairly new.

**Speaker B:** And I was sharing this with, ~~you know,~~ one of the, ~~uh,~~ folks there that are working on from the government'pers respective. And they were saying, unless I know why this is the case, I cannot take this into account. And for that, we do pay our pro forecasters to come up with better forecasts with their explanations as well. And, ~~um,~~ we place people in jobs in the past. ~~Um,~~ so there is, in a way, the leaderard can translate into, ~~uh,~~ financial incentives. **Speaker A:**  ~~Yeah.~~ **Speaker B:**  Opportunities happen on the substrate of the question. ~~And that I think is quite.~~ **Speaker A:**  Okay, cool. Can you say a little bit more about how you create that community forecast out of you kind of touch on this briefly, but this is one of the big questions that I've had, ~~um,~~ honestly, for a long time, because there are two kind of. I think of them as, ~~like,~~ canonical AI questions around AGI timelines that I've come back to for years now. I think I first tweeted about that, ~~like,~~ almost two years ago. And these are the weak AGI and the strong AGI. And we can unpack this a little bit more in a minute. But one thing that I've always ~~kind of~~ wondered is, ~~like,~~ exactly what am I looking at when I'm looking at this? ~~To what degree am I seeing, you know,~~ is it a naive aggregation? ~~Um,~~ because you can go in there and put a whole distribution. ~~Right.~~ So I'm interested in your comments on that, too. ~~Like,~~ I don't have a great sense of what I'm doing when I'm, ~~like,~~ actually drawing a curve over the potential timeline of something happening. ~~Um,~~ I feel like I'm a little bit like, ~~you know,~~ I mean, I guess I'm always ~~sort of~~ viing with these predictions, but that brings it home to me where I'm like, boy, the precision here feels like I'm leaving something behind. That is, ~~you know,~~ it's weird, right? It's a way to express my uncertainty, but it's also a way to, ~~like,~~ be very specific about my uncertainty. And even that I don't feel like I really have so interested in ~~sort of~~ how you see that kind of curve drawing. And then how are you aggregating? All these curves into one curve. ~~Like~~ do people get UPW weighted if they're better? Is there a recency bias, et cetera? **Speaker B:**  So, ~~um,~~ couple different parts there. ~~Um,~~ with respect to the community prediction calculation, we basically keep the most recent prediction from each forecaster that we have. And, ~~um,~~ we wait each forecast based on how recent it is before being aggregated. And ~~um,~~ we also, for the Met tacless prediction, do pay attention, especially if there is you, a paid client that is looking for a specific outcome or higher rigor. We ~~do~~ do things like pay attention to the past track record of each of the forecasters and aggregate that information in as well. ~~Um,~~ for ~~binary~~ question, the ~~commuter~~ prediction is basically a weighted mean, ~~um,~~ all of the individual forecasters probabilities. And if you have a numeric question or a date question, it is a weighted mixture of all of the individual forecaster distributions. ~~Um,~~ there's actually a lot of different philosophies of aggregation and this is one of the spaces that I am interested in experimenting further. These are the ones that we use. And in general, the community prediction works and seems to be really accurate. ~~Um,~~ there may be a couple of folks that seemingly know consistently beat the community prediction. And I think, ~~you know,~~ there's a couple posts, ~~um,~~ from like astral code, ext ten, et cetera, that has focused on exploring this. ~~Um,~~ I think that I'm very interested in this. Very soon we will open source metaculous. I'm curious for people to come up with, ~~you know,~~ hey, here's a difference scoring mechanism, an aggregation mechanism that actually seems to work better or seems to be more instrumental. ~~Um,~~ I really want to encourage experimentation on the space. What we have so far seems to work. If you just copy ~~the~~ community prediction, you will do pretty well in, ~~uh,~~ tournaments, ~~but then you're not really bringing in independent information, right? So~~ the better question to do is for someone to do their homework on, ~~you know,~~ what are past forecasts, ~~um,~~ that might be similar to this and then contribute, and only after that you see ~~the community prediction because otherwise you start messing with the wisdom of the crowd.~~ You said, ~~you know,~~ you might be doing vibes based reasoning. I think a lot of more. So, ~~you know,~~ reasoning based forecasting ends up being space for a lot of people.



**Speaker B:** Um, but if you've been zooming on specific moments where an entity, be it a federal agency or a person, was able to take action from a forecast, I think those are the moments where I see, yes, metaculous was actually useful or interesting. Like for example, in epidemiology, metaculus has outperformed both panels of experts on Covid and also informed hospital resource allocation, public health decision making. ~~Uh,~~ a lot of the forecasts were more robust and accurate than baseline models in predicting COVID vaccine uptakes. And, ~~um,~~ I do think that'quite interesting. ~~For example, um, and another part is on the monkeypox outbreakntaculous, um, was able to quickly provide perspective on that front.~~ Um, for example, in January 2020, back when you. Conventional wisdom was that Covid wouldn't be significant. Metaculous instead was predicting that more than 100,000 people would eventually become infected with the disease. And, ~~um,~~ a lot of folks took that as, ~~you know,~~ an early warning sign. I remember reading a post about someone making a bunch of investment decisions because metaculous said so. ~~Um, like~~ in other cases around predictions around, ~~uh,~~ Russia invasion of Ukraine, ~~um,~~ there's a comment on ~~metaulus~~ that I really like where, ~~um,~~ the metachulous user in Ukraine said, just want to say that I moved from Kyiv ~~v~~ Toiv on February 13 entirely thanks to this prediction thread and the metaculous estimates. ~~Um,~~ like that to me is a moment where, yeah, the forecasts were instrumentally useful for someone making a decision. Like we've been, ~~you know,~~ ced in the economist, our world and data, Bloomberg, Fortune, Forbes, you name it. I think the core question is around how will these be able to turn into instrumental actions that one can take? And I'm particularly interested in this being helpful for the, ~~um,~~ AI sphere and making sure the tools that we're building, ~~um,~~ on AI as a whole are actually serving humanity. And I, ~~um,~~ think if there is something that has good track record and ability to be able to influence individual decisions, we should explore why and how this can be much more useful. ~~Um,~~ and that is the lens with which I think, ~~uh,~~ prioritizing AI is important, but also other cause areas. Some folks say, ~~you know,~~ Mettachul should purely focus on long term miss causes. I actually do think a lot of short term intervention modeling and being able to take successful action teaches us on how forecasting can be helpful in longer term contexts. Like questions like, how can we reduce homelessness in San Francisco? If I have an additional 5 million, should I just go build two more houses? Is that the best thing I can do? Or is there a form of lobbying that I do think a large crowd will say this will make the housing market understand the negative externalities of, ~~um,~~ homelessness, to the point that we can have a more well grounded change. ~~Um,~~ I would like this to be a minitaculous where every forecast, these forecasts can be conditional, they can be intertwined with each other. We can explore different ways to visualize them. Like these will bring usefulness towards someone, and us being good at this in a repeata fashion will help us be able to tackle the largest questions much better as well. **Speaker A:**  Cool. Very, ~~uh,~~ interesting. So going to these two AI questions that I refer back to the most, I think probably many listeners will have visited these pages. The weak AGI and the strong AGI timeline. Each one has kind of four different resolution criteria. And the question is basically, when will a single AI satisfy these different criteria? And you get to predict your distribution of dates. **Speaker B:**  Right. **Speaker A:**  I think this has been really interesting and quite informative for a long time, although more recently it does feel like it also highlights a real challenge of writing these questions, which is that when things are farther out, it seems like ~~the.~~ ~~Or it can seem.~~ I feel like in this case, it does seem like the detailed criteria, ~~you know, seemed quite reasonable.~~ And now as we're getting closer, I'm feeling like there's ~~sort of~~ a divergence, ~~uh,~~ between what matters and what is actually ~~the, you know,~~ the letter of the law in the question, particularly when it comes to in the week agei question, the use of a specific form of the Turing test where I'm ~~kind of~~ like, okay, from my standpoint, in the ~~sort of~~ intuitive, ~~like,~~ what really matters in the Turing test line of thinking, I would say we've passed it.

**Speaker A:** And yet the formulation requires an, ~~uh,~~ expert interrogation and ~~that~~ expert not be able to tell what is the AI and what is not the AI, and we're not that close to that. But I always emphasize for people that I think the reason we're not that close to that is a design decision of the people that are making the AI's that are to be tested. **Speaker B:**  Right. **Speaker A:** ~~Like,~~ if I were going to try. **Speaker B:**  To pass, ~~uh,~~ how confident are you about this claim? **Speaker A:** ~~Um,~~ I would say quite in as much as if I wanted to create an AI that would be impossible for a. Or much more difficult for a interrogator, ~~um,~~ to identify as AI or not. The first thing I would do would be have it say, I don't know, a lot more often than it does, right? ~~So,~~ like, the easiest way for me to tell whether something's an AI or not is just to ask ten very long tail random questions and be like, ~~no~~ human is going to answer these questions all ten. Like, it's just wa. ~~You know,~~ nobody has that breadth of knowledge. Actual people would say, I don't know. So I would just train, ~~you know,~~ I would actually dramatically narrow the, ~~you know,~~ the range of responses from the AI and make it seem much more conversational, make it seem much more ignorant, make it much less useful, ~~you know,~~ for what you would actually go to chat GPT for. But I think I could make it a lot harder for the expert interrogator to figure out what is what. So anyway, that's like just one example of this general problem. ~~Um,~~ I ~~wonder,~~ wonder how you guys are thinking about that. **Speaker B:**  So to me, the changes you mentioned, I'll start from what you just said and then go zoom back to the hi question. The changes you mentioned could probably be enacted by just using a couple language models that are policing each other, and you could basically get to a system that actually behaves this way right now. ~~Um,~~ so I'm not very sold that know that is the main threshold here. ~~Um,~~ there is a quote from Jren Lanier that I actually used at the beginning of my thesis that was focusing on how can we augment citizen, ~~um,~~ participation in governance, ~~uh,~~ through natural language processing. I guess today I would call itms, where he says, ~~um,~~ the Turing test cuts both ways. ~~Um,~~ if you can have a conversation with a simulated person presented by an AI program, can you tell how far you've let your sense of personh who degrade in order to make the illusion work? I think this is quite important here, like, being able to communicate on a certain pattern full of expectations is not what I am interested in. I think that's a ~~very,~~ very low bar. In fact, like the goal, my interest goes way beyond am I able to simulate something that is convincing enough in this closed box tournament like this does not actually bring us a better world, from my opinion. I actually agree with you. I don't really love the way the question is operationalized, but keep in mind, they are both from 2020. ~~Um,~~ we will not always formulate questions so that they are optimally informative years later. ~~Right.~~ And I would say that in 2020, these questions I you'said, it to yourself, were in fact quite useful to the point that I would argue attackersus probably moved the Overton window with respect to how people were paying attention to, ~~um,~~ the impact of AI. And I think that is one value add that's both hard to track, ~~but~~ also intuitively resonates with me, which is what also draws me here. So I think these questions somewhat serve their purpose. Now, zooming out of the purpose question to like, okay, but can we have a better question? If we were to do this today, these questions, instead of it being a single question, I think this should be shaped something like the ~~minitaculous~~ instance that we were talking about, where all of the subpars of the questions would have multiple different forecasts and they all together in an ensemble, actually give you a picture of. Heck, we don't even have a clear definition of AGI to the point that the question needs to operationalize itself one way or another. ~~Right.~~ One of the, ~~uh,~~ projects that I'm ~~t~~ pretty excited about on metacas is to come up with indexes, basically come up with just the way you can ~~aggreate~~ a bunch of stock tickers to come up with a composite view.

**Speaker B:** I would like there to be 30 forecasts that are all really zooming in on slightly different aspects of this. And them allogether is what you're paying attention to now. You can say, well, I want to rigorously link all of these through causal diagrams, and then you will get into a whole different hair ball. Sure, that could be helpful if you pull it off, but even a lesser fidelity version of this that is just here is 30 of forecasts that all rhyme with each other with respect to its focus point will be able to shed more light. And I think at that point, with what metaculous had, ~~like~~ that was possible. And these questions did serve their purpose. ~~Um,~~ like, it's a challenge of forecasting the future. ~~Right.~~ We don't always know what formulation for a question will be most useful years from now. ~~Now, um,~~ for example, we're doing the five years after AGI, ~~um,~~ question series that just launched. ~~Like,~~ I am much more interested in that, for example, because when I look at the answers there, it actually gives me a much more accurate view of what people even conceive of as AGI. ~~Like,~~ the way I am using those questions isn't about thinking of, okay, ~~uh,~~ will these things happen when AGI hits it more so informs me, okay, these are the things that people consider as critical possibilities with an AGI or not. And just doing a throwback to some of my prior work with AI objectives institute, I often find there to be, if we are talking about AGI and AI alignment without talking about the societal context in which this has impact, I think we are narrowing this down. ~~Like,~~ for example, there's a common meme that we would use in AI objectives institute to talk about what successful AGI that can enable human coordination is in the world. That is post AGI. If you don't think Jerusalem will be a united and peaceful city, maybe your AGI isn't ambitious enough. Like, maybe something that is truly AGI would be able to resolve clashes and cruxes within existing human sphere, where it's not trained to make sure it is perfectly neutral towards that, but instead it augments human agency that actually brings a meaningful shape. Because otherwise, ~~you know,~~ sure, I canh up a system so it can say, I don't know, enough time so that you might find that convincing. I find that to be a lower. I think we should be more ambitious. **Speaker A:**  ~~Yeah.~~ So what do ~~you,~~ how much editorial do you exercise over these questions? Because it does seem like these couple of questions have ~~sort of~~ become a bit of a shelling point. And it's a delicate, ~~um,~~ decision probably from your perspective, right? Do we sort of take a proactive step to retire a shilling point because we feel like it's outlived, ~~uh,~~ its usefulness or do we just let the community kind of gradually move on? **Speaker B:**  I wouldn't say that at all. I think like these questions are good. I am much more interested in pushing it further. ~~So uh,~~ I guess it's also ~~uh,~~ a call to action slash ask for help. For everyone that loves metaculus is interested in this. As we launch minitaculus, which are focused instances, like if you have different formulations that you think are interested, come tell us, ~~like,~~ we already get a lot of inbound questions, ~~right,~~ that are around by Ky. Can we write a question? We heavily editorialize them for we do want to maintain a level of rigor. And so far I think metaculous has been much more close than what I would like it to be. I would love for there to be many more people. That's right, questions, especially domain experts, especially people who say, hey, I get the hang of what a question is resolvable and meaningful and these are things we can help with. But we are entering a chapter of metaculous as we are open sourcing to get many more people to write questions, to have instances that they are hosting that have maybe different scoring mechanisms, ~~you know,~~ different levels of rigor to the point that it actually proves useful to them. We will also host ataculous.com, many instances that are very domain specific. So I just think we need more questions. It's not about you going back and changing what this has accomplished, but more so bringing on new lights. Like for example, I'm quite excited about a bunch of new technologies that could actually bring better results. ~~Like~~ I love a lot of the, or, you know, symbolic base.

**Speaker B:** Like are we able to use language models in order to ~~um,~~ come up with specification criteria and ~~um,~~ use that to create safeguard AI systems? ~~Uh,~~ I'm interested in singular learning theory. There's a bunch of new techniques that actually, when I imagine versions of AGI or you, weak levels of AI competency, even that is really good at these, the ways in which I envision those features are substantially different. And I am quite, ~~quite~~ interested in exploring these. And I think that just requires a, ~~ah,~~ breadth of more questions to forecast and also more human energy or AI energy to be able to forecast on these questions. So anyone that is interested in saying, hey, I have things I want to forecast, I have a formulation that I think is better, come talk to us. **Speaker A:**  How do you create density though, right? ~~I mean,~~ the one worry that I would have if I was you and I got a million new questions in, is now I need ~~like, you know,~~ a billion predictions, ~~right,~~ to actually create a meaningful community forecast for all those million questions. So how do you think about balancing the diversity of questions versus the density of forecasts? **Speaker B:**  ~~Right.~~ Great question. **Speaker A:**  ~~Um,~~ this is also where the AI's can come in. We'll get there momentarily. **Speaker B:**  Yeah, the nail on the head, like the real currency here, the real limitation is human attention bandwidth. ~~Right.~~ I have 10,000 questions being launched at metaculous every day. It's not going to help anyone, at least until everyone is forecasting part time as part of their job. That's not what I'm trying to angle towards here. But this is why we need to do active research on questions like, can we build indexes that aggregate a bunch of different forecasts? Can we have AI start forecasting as part of, ~~uh, uh,~~ AI benchmark tournament? We've had an explosion of AI contributors. They seem to be doing okay. And can that create a scaffolding or jumping off point for humans to be able to get to a rigorous forecast much faster? ~~Um,~~ there is, I think, a lot we can do to increase the quality. ~~Um,~~ the wind condition isn't, we have 10,000 new forecasts every day. I think that just will drown any kind of quality from the system. And the wind condition also isn't, well, AI's are forecasting and humans are just watching. I think the question is more around are we able to identify what are the best questions? And that requires, I want that to happen with more people, not just you, the metaculous team. And I want the contribution of the bot to be able to gear towards shedding more light and more information and have these questions be incrementally composable, ~~um,~~ so that we can build world models of, ~~you know,~~ all of these are rhyming with respect to how we are thinking about electric vehicle proliferation, for example, ~~you know. Um,~~ and using those, then we can have much vaguer questions like, are we able to, ~~you know,~~ like forecasting specific windows of electric vehicle proliferation in China is quite specific. But if I have 30 of those questions, I can then make something aggregate that says, ~~you know,~~ will electric vehicles be better for the environment? It's like, what does that even mean? But like, it actually means a compost of all of these different things. This is a different frame of reference for thinking about how we can aggregate them. **Speaker A:**  Cool. ~~Um,~~ well that's great motivation then to get into ~~kind~~ of what the state of the art is in LLM powered forecasting, and then the tournament that you're running to try to advance that state of the art a little bit further. In preparation for this, ~~uh,~~ I read through three links to papers that you had shared, and I was overall pretty impressed. You can give me more detail on ~~kind~~ what you think is most important or stands out, or what the kernels are that you really want to build on. But it seemed like, across the board, pretty positive results. And these are from serious, ~~um,~~ authors, including Tetlock, has been involved in some of this research. Just to summarize the three papers, the first one was approaching human level forecasting with language models. And this was from, ~~uh,~~ Jacob Steinhardt and his group. They basically created, I guess, what I would describe as ~~sort of~~ the intuitive thing that I would create if I was going to work hard on trying to make this work. And that is like a retrieval system, the ability to go on the Internet, search through the latest news process that, ah, um. And ultimately create forecasts.

**Speaker A:** And it seemed like it was pretty good. It was like coming close to the community forecast, although not as good as the overall community forecast. Um, a question that I did have reading that paper, which I don't know if you would know the answer to, is, okay, it was a little bit short of the community forecast, but, like, how does that compare? If it were an individual human, ~~you~~ what, at what percentile would, ~~um,~~ that system have performed? ~~Uh,~~ I don't know if you do know that, but's my intuition was that it would be pretty high as like, the percentile of individual humans. **Speaker B:**  Right. ~~I mean,~~ let me give you a thing that I do believe in, that I don't have data, but ~~I think~~ we will soon have data for this. Is that part, in the hands of a team of human forecasters will just kill it. Like, that is obviously going to be much better. ~~Right.~~ Going back to the question of usefulness, I like the academic rigor aspect, don't get me wrong. I do think testing these in isolation is ~~good.~~ And a lot of our designs around, ~~uh, um,~~ AI benchmarking for, ~~uh,~~ competition pays attention to that. But the part that already is obvious to me is let's actually look at what these systems are good at. They're good at going through massive amounts of content, identifying which parts may be relevant. They're good at taking a first step at, um, creating a world model. ~~Um,~~ let's actually build systems that pay attention to those first, ~~um,~~ and build on top of that. ~~Um,~~ I particularly like the sch. Steinhardt lab. The Halloween paper for a lot of intuitions that I would like to explore further is in that paper. And a lot of folks that have asked me, I want to compete in the tournament. What should I do? I always point them, ~~well,~~ start from here and then try different things. Try different ensemble methods, use different language models. ~~Um,~~ share prior data, don't share prior data, have them debate each other, have a third model, look at it and synthesize them. ~~Um,~~ there's a lot of playful strategies that one can go with. So I really enjoy thinking in this framework. ~~Um, so I've been encouraging, um,~~ there's a very active discord, by the way, for the, ~~uh,~~ tournament where people are discussing strategies. It was absolutely delightful to see how much the models have evolved in just ~~like~~ the three weeks of the tournament kicking off on, ~~um,~~ different strategies people have come up with. And we do encourage people to update their models. ~~Um,~~ for the tournament, there shouldn't be human in the loop because ~~we'trying~~ to benchmark the state of affairs. But if they have a better strategy, they should update that. ~~Um,~~ another thing we do is reasoning transparency. Have the models post at least some form of text we don't grade or score this because it is a whole different level of complexity. For that, let's stay with the forecasting accuracy. But that at least gives us a way in which how the model is thinking. And a lot of earlier research on chain of thought reasoning has pointed that, ~~you know,~~ explanation is actually what gets these models to be able to stay further grounded. So, to me, these things are really good. ~~Um,~~ couple other avenues that I don't think has been explored as much lately is how can we bring in model based reasoning on top of language models that will actually yield ~~much,~~ much better results? Even in the first few weeks, we have, what, 15 questions that have resolved so far? ~~I think, um,~~ in that signal, we've had plenty of questions that have, for example, tests like scope sensitivity or, ~~uh,~~ negation. ~~Um,~~ ask the same forecast, ask the opposite. See if the answers are the opposite of each other. ~~Uh,~~ we've had examples like, for example, there was the measles question. ~~Like, if you ask the bots, you know,~~ what will be the number of know measles cases? Less than, ~~uh,~~ 200. It'll say 70%, 75%, 70% for more than 300. And that means, ~~you know,~~ there should be five chance that it will be between 203 hundred, and bots will say, ~~you know,~~ 65% for the window that's between 203 hundred. ~~Like,~~ obviously, a pro forecaster wouldn't make this mistake. But even an ensemble method falls apart on this case because it doesn't keep track of. Here is a world model that is mathematically rigorous. Now, why is this interesting to me? Humans wouldn't also be able to do a very good job at tracking this if they were much more complex.

**Speaker B:** ~~Like,~~ obviously a pro forecaster is trained to get better at it, but if you ask this to someone that is, you know, not highly numerous child, they would be ~~like,~~ oh, maybe it's the same. Oh, I didn't realize that this is wrong. That means there's something intuitive about how humans reason that is distinct from the mathematical rigor here. How can we close that gap? ~~Like,~~ if we are able to incorporate model based reasoning? ~~Uh,~~ and I think this can happen with approaches like open agency architecture, or even like a simple squiggled integration. I'm super interested in that. To be able to say your forecasts as you bring in more and more information from a language model, is building a world model that also has a mathematical grounding in which you can enforce things like scope sensitivity, ~~negotion, uh,~~ negation sensitivity, ~~et cetera,~~ et cetera. There. I think we will actually have a lot more paradigm shifts. And this goes along with an intuition that I would like to be much more explored for, ~~um,~~ building safer AI systems, which is, can we have language models to be used in order to come up with rigorous world models that can be interpreted because they are mechanical? And, ~~um,~~ there are papers on this, ~~ah,~~ front too, ~~um, um,~~ that I think have explored this in different ways. For example, there's the Tenenbaum lab at Stanford that's published a paper called from word models to world models was, it's about a year ago. ~~So there's plenty of work that come on top of that.~~ Are we able to build, ~~ah,~~ probabilistic, uh, ~~uh,~~ programming language, ~~uh,~~ support? ~~Um,~~ so use an LLM m to come up with a world representation where the world representation is both consistent and also interpretable. ~~Um,~~ can we use this as a starting point, ~~um, to come up with, uh, uh, to much, um. Um, better,~~ much more robust worlds of, ~~um,~~ AI systems that are getting more accurate, that are getting more logically consistent? ~~Um,~~ I can go a little more technical if you're interested. Like for example, can we come up with auto formalizing option spaces? ~~Like~~ there's a finite set of proposals, there's a constant positive number, that is the total budget. Say, we're trying to do like a question of budget allocation. The option space has many finite functions that, ~~you know,~~ the sum of all over the actions needs to be less than or equal to the budget. ~~Like,~~ an LLM is not going to parse that. But an LLM might be able to get better to this. If you're thinking about, ~~um,~~ take a natural language description of a decision situation and produce a formal description of the decision variables, the constraints in the language optimization framework, ~~um,~~ ton of research, like use something like cvxpy, like there's a bunch of libraries that would get this better. We can do structured construction of option spaces. ~~Like,~~ are we doing a simple choice question, are we doing a matching problem? M like LLMs would actually be good at figuring out. ~~Oh, uh,~~ the thing you're asking me seems to match this kind of problem and then help parse into that. That way we actually have a world model that we can inspect, that we can see, okay, these parts are right, these parts are wrong. Can you improve this and have the human continue with just verbal intuition rather than, ~~um,~~ need to keep an excel spreadsheet to see if all of my intuitive probabilities are tracking, but instead you're able to say things like, these are my intuitive probabilities. Can you figure out where I am? Logically not consistent like, this would help a forecaster right now. Here's all the data that I have. Here's 30 forecasts. Maybe these 30 forecasts have some logical inconsistencies between them. Can you do better? If we can help language models get better at this, that's composite system, I believe will yield a much more reliable, much more safer AI model as well. And if we extend that further and further, ~~um,~~ we actually might end up with AI systems that are interpretable, that have reasoning transparency, that have distinct parts of world model building, exploring option spaces, eliciting preferences and desirabilities from people that you can look at and see, ~~uh~~ oh, this is how all of them are talking. If you're just trying to get one LLM or a bunch of LLMs, not so much like, I will never be able, but maybe through breakthroughs on mechanistic interpretive, we might be able to get there. ~~Um,~~ so I believe these kinds of explorations will actually yield much better AI forecasting. Yeah. Cool. **Speaker A:**  So that's awesome.

**Speaker A:** I want to, just for closure, for people who wanted to maybe hear those other two papers, I'll just mention them briefly, ~~uh,~~ because I think if you're going to get into this tournament, you should look at the, ~~uh,~~ Halloween and Steinhardt paper for sure. That's like you're ~~kind of~~ jumping off point. Well done, agent with retrieval, with fine tuning, with, ~~ah,~~ a good scaffolding to spit out good predictions. That's the one that I mentioned before, comes reasonably close, but still fall short ~~of the,~~ of the community prediction. Then there are two from Tetlock and co authors. One was pretty interesting around just giving people access to an AI assistant and seeing how that helped them as humans forecast. This is kind of the big picture, at least a small step toward the big picture vision that you're painting. Interestingly, ~~they had a biased,~~ they ran an experiment where they gave the best assistant that they could give to the participants and then also a deliberately biased, ~~um,~~ AI assistant, and they found that both helped, although the biased one didn't help as much. ~~Um, that's an interesting finding and also an interesting argument in that paper about how this look at the future.~~ ~~I mean,~~ you could think ~~like~~ multiple reasons for why AI forecasting could be useful. You've made the case that, ~~you know,~~ obviously we would like to have a better sense of what's going to happen. We would like to be able to make better decisions. They also make ~~kind of~~ an argument in that paper that the study of what language models can predict is also ~~kind of~~ a useful way to interrogate to what degree they can get out of distribution, which is a very hotly debated topic within the study of AI. ~~Right. Some level.~~ By definition, if you're predicting the future, you're out of distribution. So that's, I think. And the fact that they are doing it reasonably well is definitely at least some points for team. These things can generalize, ~~um,~~ purely beyond their strictly defined training data. ~~Um,~~ then the third one, also from Tetlock and a number of the same authors, is just an exercise in ensembling ~~the different language models and finding that like~~ the average of the language models performs better than the individual. So it's kind of wisdom of, in fact, ~~that's the title of the paper is wisdom of the silicon crowd.~~ Yes, that one was interesting to me. ~~I mean,~~ they did as you'd expect from a Tetlock publication. They pre registered all their experiments and were very ~~sort of~~ rigorous about, ~~you know,~~ declaring exactly what hypotheses they were going to test. It did jump out to me from the data that GPT four was like way outperforming the other models. And if you wanted to make a really simple improvement on their method, I would just cut the worst models and take, ~~uh, one or~~ like a few of the very top performing models and just ~~kind of~~ run multiple copies of those. **Speaker B:**  Right. **Speaker A:**  ~~Um,~~ a couple interesting notes there around bias toward round numbers, a little bit biasd toward positive resolution. Some of these things that you noted where there was ~~like~~ some logical inconsistencies and they ~~were also hard.~~ Another note I had on that paper was, it really shows the breadth of AI knowledge off in a powerful way because ~~I would have had to do. I was looking at some of these things, and I'm like,~~ I don't even know the person. It might be a leader of a country or whatever, and I don't even know who that is as we begin this, ~~uh,~~ discussion. So the fact that the a as are just jumping in and making, ~~uh,~~ predictions on such a wide range of things is ~~like~~ a good reminder of some of their fundamental strengths. **Speaker B:**  ~~Right?~~ **Speaker A:**  ~~So, ok,~~ that's all background and hopefully breadcrumbs for people that want to get into the tournament. **Speaker B:**  You've alluded to the tourn. If I make comment on then a little bit, I think the conclusions of these papers do make intuitive sense, and I'd love to see much more rigorous tests of this. ~~Like,~~ for example, what you said on, well, cut the worst models, and maybe that'll get you better. Maybe.



















**Speaker B:** I recommend the audience to check it out. ~~Um,~~ website is AI do objectives dot institute. And they can see on blog post a couple case studies, ~~um,~~ that we've done on, ~~uh,~~ impacts of talkk to the city. Like, for example, one was with Taiwan. ~~Uh,~~ we've had an extensive collaboration with Taiwan's, ah, Ministry of Digital affairs, ~~um,~~ where we were aggregating people's opinions with respect to both local municipal questions and also larger scale questions, ~~um,~~ on, ~~you know,~~ same sex marriage data. A lot of these data inputs come in from polis. Now, Polis doesn't like, there's some constraints there. That is a trade off, or it ultimately is a recommender system where you're, know, finding people that will share certain viewpoints instead of leaving the onus to that which causes, ~~you know,~~ less human bandwidth, like, you can only write about tweet sized things. We said, okay, can we have just completely unbounded input window where people can send in text, they can send in videos, they can share any kind of data, ~~um,~~ that you, you would be able to then enhance and see, okay, what does this community talk about? What matters to them? And in a way, my journey to forecasting into metaculous was me seeing, okay, we are actually at a place where we can aggregate public desirability. ~~Um,~~ I prefer the word desirability to preference because preferences can change. One might not necessarily be aware of their preferences, as you can have cyclical dependencies. I use desirability more as a catch all term that overcomes some of these. We can aggregate these, we can have a snapshot. Like, one project that I loved, for example, was with the ~~va, um, the, um,~~ labor union, ~~with, uh,~~ focusing on, ~~uh,~~ veterans health, ~~um,~~ where, ~~um,~~ they wanted to run a survey with respect to a specific proposition. And the negotiation, ~~um,~~ windows they have with the government is incredibly small. There's a 20 day window where they need to come up with a policy, share it, and it will go through, ~~um,~~ the whole system is basically rigged against internal deliberation. Just, it's so narrow and so fast paced that people just say, whoever I voted for as head of the union should go forward. The new president of the union, Mark Smith, ran a survey, ~~um, open ended,~~ text based question to the entire population. Within 24 hours. I think we got, what, like maybe 200 responses on the first round. Within 24 hours. We could turn this open ended survey in multiple languages, by the way, not just in English. You can get a bit Spanish and Tagalog and consolidate this to say, here are the four viewpoints that came up. What are your thoughts? Send it back to the entire community and develop this recursive loop. It's so much more interesting when you say, wow, between just yesterday when the survey went out, I already have four clusters. Maybe I should respond to this. ~~O uh,~~ I have something to add to this one. Next day, do it again. This cycle is much cheaper than what it used to be. I think this is a paradigm shift like, this changes governance. And what I realized is, okay, now that we can aggregate this, what I need is the next pillar, ~~um,~~ which is, will the action over actually get us towards where people want to go? And that's. I was talking about this with Gaia and that it was the previous year of metntaculous, and that's how I ended up here eventually. So I'm really excited about seeing how forecasting can help this. I think there's a lot there, and I think we need a lot more experimentation. ~~Um, so, yeah,~~ like polis, like tools is in a way where I started thinking, huh hu maybe there is a way in which we can do better. **Speaker A:**  Glad I remember that, ~~uh,~~ bonus question. I think it does sort of start to look like a liquid democracy or a sort of technology mediated liquid democracy, and that is definitely super compelling. ~~I mean,~~ there's just not much room on. We see this right now as we're going through this campaign. Obviously, in the US, it's like the things that are actually getting talked about are few and not particularly well chosen in many cases. And, ~~uh,~~ you know, there's just so much stuff that really ought to be, ~~you know,~~ that we want to understand, ~~you know,~~ what people actually care about, first of all, or how they think about different things. We're just not even getting that data in the first place, let alone, ~~you know,~~ being able to map a path toward actually delivering for people.

**Speaker B:** **Speaker B:**  Ye so, absolutely, absolutely. And I think, ~~you know,~~ the bit signal of you red or blue votes in the US, we can do so much better than that. That's why I'm looking at municipal engagements, membership based organizations, looking at daos, for they are making a lot of decisions around, ~~you know,~~ their stakeholders, and the entire thing is on a digital substrate, so there is much more experimentation here. What you said reminded me of something with respect to, ~~you know,~~ liquid democracy. I must say, ~~like~~ the term liquid the Mar democracy does bring some aspects of future in my mind also, and I think it's worth pointing that out. ~~Um,~~ there's this concept that I call the consensus illusion. ~~Um,~~ there is a drive towards the lowest common denominator that if we say finding consensus in a community is desirable, for that is the best policy, what you will end up is a lot of policy outcomes that are quite lukewarm, that don't actually address the issue. ~~Um,~~ there was a Europe's paper from DeepMind and, ~~uh,~~ like ~~2022~~ on find, ~~like,~~ how can we fine tune language models so that we can find agreement across humans that have diverse preferences? And one of my concerns when I see work like this, I think the paper is actually great. ~~Like,~~ some of the techniques they used has made its way to talk to the city. So I like that. ~~Um,~~ some of the concerns I have is finding agreement doesn't necessarily mean good policy. In fact, it quite often doesn't. Like on an example I like to give is one of the conclusions from a civic data set. ~~Is,~~ um, we should build more bike lanes for the community. Everyone seems to on to our back lanes, and you say, okay, where should we build these back lanes? It should it be on this street? And the model will say, ~~no, no,~~ no, that's not what we agreed on. It shouldn't be on a specific street. We need more bike lanes. This is the thing. We don't want to make any specific streets narrower. And it's just like, oh, we're doing the same thing we've been doing with politics all along. Where find the most common denominator statement that, ~~um,~~ yields power. Because in the current of elective system, consensus is power. So when you have a desire towards consensus, that causes a trade off with fidelity to your viewpoint, and if you seek power over fidelity, you will have higher representation of a viewpoint that is not very meaningful. And, ~~you know, the most,~~ you know, catch all sentences will end up resonating the most, ~~um,~~ and it won't make meaningful policy. So I don't want consensus, really. I want to start from a shared world model. Like, can we actually come up with ways in which we have desirabilities for stakeholders, we have action possibilities that come from policymakers, and then we have outcome likelihoods from domain. And these three need to be continuously talking to each other. As you division of labor, ~~I think~~acy focuses on the state desirability for stakeholders. But if the options you give is only roads, you can't really build a bridge. And people need to be able to say, I want to also be able to build a bridge and an AI model that hampers this by finding o, ~~uh, this is~~ the most agreeable thing is not good. Instead, I'd much rather have an AI model whose goal is come up with individual viewpoints that are clashing with each other, represent each of them with high fidelity, and identify what are the cruxes between these viewpoints. Like, if we can find the good crux where I think a is true, you think a is false, but the underlying cause, ~~if I thought~~ the underlying cause change, this would change my mind about a, and you would say the same thing. Like the typical, ~~uh,~~ double ~~crre~~ process is good because that means we actually have a same shared world model, ~~um,~~ and we need more data. And at that point, bring on further research, bring on forecasting. Like we're in a good mode. The failure mode here is a risky alliance where someone says, oh, I think policy a is good. You say, I also think a is good. And I say a is good because it will enable b, which will be great, and you could say, I don't think b is likely to happen as a result of a, but maybe we should keep together because it seems instrumental just for this one step. And you end up having a lot of unlikely alliances.

**Speaker B:** And the ultimate version of this is a completely polarized society where you for some reason know social conservative and fiscally conservative behavior is correlated with each other, even though that doesn't necessarily manifest. ~~Um,~~ there was one research that I read, ~~I, um, can't remember where, that~~ I just absolutely loved, which was instead of asking people their opinions with to immigration policy, it actually asked a statistical question on how many people are you willing to let in, ~~um,~~ before you reject someone that truly deserved to come into the country and, ~~uh,~~ the previous people that you have Letin might be mistakes. This is very interesting, because instead of people saying, ~~well,~~ I am liberal, I am conservative, you see people who say they are liberal, San Francisco crowd that will say, yeah, I'm willing to let in five mistakes before we admit one person. And others will say, what do you mean five? We need like 300. But both of these people see themselves as having a liberal opinion. ~~Like,~~ we do not yet operate on a level where we are looking at good policy. So we should not encourage these ~~illusion~~ of consensus so that it can serve towards better power. Instead, we can use these techniques that we have right now to go towards better policy. And better policy means higher visibility, better policy means people agree on the world models. So I see the work we're doing at metaculous as an instrumental step in that trajectory. Are we able to find people whose world models are diverging? Let's figure out why they are diverging. Are we able to see, okay, will this action that a policymaker has given us bring us to the outcome? ~~Um,~~ I think these are really important questions around the category of you. How can we have better epist security? **Speaker A:**  It's crazy to think how far this might be able to go over the not too distant future. I mean, obviously, you're primarily focused ~~right now~~ on getting a read on what the bots can do and trying to be as accurate as you can and to be much more in depth on these sort of mini metaculuses. Do you have a roadmap for let's ass assum, this works? Do you have a roadmap for how this sort of rolls out and scales up to ultimately, ~~like, big picture,~~ most important questions in society? **Speaker B:**  Right? And ~~that question I hear, ah o,~~ they are, you should write a blog post about this, probably that maps this out. ~~Um,~~ quick thoughts that come to my mind, ~~um, like,~~ we know preferences ~~can~~ and do change based on actions and their consequences, right? So can we build these feedback loops as proof points in organizations that can actually take action on them where real stakes are present? ~~Um,~~ that's why I'm interested in labor unions, for example, because it's a fairly acute case of coordinated decision making with stakeholders that are outside and inside the group. So it gives us a lot of visibility. I think where we are at right now is, I don't expect the top down revolution of a government adopting this off the bat, but I do think there's a lot of municipal level, ~~um,~~ experimentation that already has been happening. ~~Like,~~ for example, talk to the city is collaborating with, ~~uh,~~ a couple of municipalities in Japan right now, ~~um,~~ through liquid, which is a japanese, ~~um, liquid~~ democracy focus company. ~~Um,~~ there's a bunch coming in in Taiwan. We have interest from folks in Singapore ~~to be able to~~ use metaculous forecasts. For example, we have interests in similar Taiwan community. We have groups in the US also on municipal level. Like for example, Detroit has a bunch of communities, ~~um,~~ that are focusing on ~~Well-being~~ of African Americans and previously incarcerated forks. Are we able to figure out what interventions bring a better world to them? These are all very short term, but in these processes we can actually see, oh, this seemed to have worked. This actually did yield an outcome where a group was able to coalesce much better. I think we need more visibility into that. This, I think, is the very first step. I think AI tooling is absolutely critical for both the failure mode and the success mode will heavily depend on how these tools are implemented and used, ~~um,~~ how these tools are actively enhancing human aid. ##y um, I would recommend people who are interested in more of this to check out the roadmap documents from AI Objectives Institute. That's where I have done a lot of my writing and thinking with the team there. ~~Um,~~ and I reach out to them also.

**Speaker B:** Um, I still try to stay as involved as I can, but it does hold a dear space in my heart for Peter Eckersy, who was the original founder and a, ~~uh,~~ mentor and a friend of mine, unfortunately passed away quite unexpectedly, which is when I started leading AI objectives institute. ~~Um,~~ and like the line of thought, there was very much always, ~~like,~~ AI can be a transformative point for human well being, but the default systems do not place us on that path. ~~Um,~~ so I think there's a lot there, and this is the question of existential hope, right? Like, I see existential risk and a lot of risk related failures as let me put it this way. Existential risk is failure to coordinate at the face of a risk. If we can already foresee this path and we are failing to coordinate the systems we are in, the coordination capabilities we are in doesn't let us get to the heart of that. That is why we are fail. So that is the angle that I wanted to keep looking at. Because we have seen incredibly successful cases of international coordination, or multi corporation coordination. Like the ozone layer is basically recovering since we have banned CP CFcs. Like there is many different cases where we have moved mountains as society. Like microplastics related harms. Like we have bands, ~~um,~~ lead at this point. Like there are ways in which we are able to coordinate if we can create coherent world models. And I think the thing we need to do is have shared world models that can contain the disagreements rather instead of agreeable action policies. I think the way politics happens right now, voting happens right now, is find the most agreeable action policy so you can maintain control as opposed to, say, what is the good policy? And this requires the level of epistemic rigor and epistemic security. I see. My life's work is to focus on that question and bring more and more towards that. And if there are groups that we can work with, hell yeah, let's kick it. This is where we need to start. So if there are any organizations that are focusing on our priority cause areas, know, be it on ~~um,~~ climate, ~~uh,~~ change, or AI, or like nuclear consequences, or there's any organizations that are trying to do better on resource allocation where they want the resources to be able to do the maximal good for this specific community, I would love to talk to them. I would love to understand how ~~the things~~ we are building can be useful for them. ~~Um,~~ I think we need more experimentation of this sort. And, ~~um,~~ I'd rather have these experiments be with people that benefit from them in the immediate short term. Cool. **Speaker A:**  Well, that's a great, ~~uh,~~ call to action. ~~Um,~~ I'm glad we stayed on the little extra to get, ~~um,~~ that final section. ~~Um,~~ I guess I'll ask again, anything else that we didn't last time? It was a fruitful question. Anything else that we didn't, ~~uh,~~ get to that you wanted to touch on? **Speaker B:**  Nope. I feel complete. ~~Um,~~ thank you so much for this opportunity. It was lovely for it also made me realize the context through which my path has evolved. Like seeing the role forecasting can play hand to hand with collective intelligence and the failure modes of, ~~you know,~~ the current political processes or democracy or resource allocation makes me realize, oh, I see the role at this place. ~~Uh,~~ so this was great for me as well. So thank you. **Speaker A:**  Cool. Thank you, Dar. ~~Uh,~~ make sure I'm saying it correctly again. Dr. Tan, right? **Speaker B:**  Yes. **Speaker A:**  Der Tan, CEO of Mettaculous. Thank you for being part of the cognitive revolution. **Speaker B:**  Same here. **Speaker A:**  ~~Uh,~~ great job about you. I think, ~~uh,~~ some of your commentary, especially toward the end there, and think it'super fascinating. I'll hit stop.

